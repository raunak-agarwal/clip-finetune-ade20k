{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/ADE20K-pairs/captions/ade20k_train_captions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/ADE20K-pairs/captions/ade20k_train_captions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib, numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda:1\")\n",
    "   print(\"Running on the GPU\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "   print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-question-generation-ap\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 269 ms, total: 2.38 s\n",
      "Wall time: 2.38 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How many people are sitting on chairs in the picture?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "def get_question(answer, context, max_length=32):\n",
    "    input_text = \"answer: %s  context: %s </s>\" % (answer, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt').to(device)\n",
    "    output = model.generate(input_ids=features['input_ids'], \n",
    "                            attention_mask=features['attention_mask'],\n",
    "                            num_beams=10,\n",
    "                            no_repeat_ngram_size=2,\n",
    "                            num_return_sequences=1,\n",
    "                            early_stopping=True,\n",
    "                            max_length=max_length)\n",
    "    questions = [tokenizer.decode(o) for o in output]\n",
    "    return questions[0].strip(\"<pad> question: \").strip(\"</s>\")\n",
    "\n",
    "context = \"This picture describes about inside view of a hall, in this we can find group \\\n",
    "of people, few are sitting on the chairs, in the background we can see lights.\"\n",
    "answer = \"not many\"\n",
    "\n",
    "\n",
    "get_question(answer, context)\n",
    "\n",
    "# output: question: Who created the RuPERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = 'In this image there are cushions on the bed. \\\n",
    "# Below it there are baskets. Background there is a wall having windows. \\\n",
    "# From the windows few trees are visible. Left side there is a blue curtain.'\n",
    "# answer=\"blue\"\n",
    "\n",
    "# get_question(answer, context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 'there is a shelf'\n",
    "# a = 'yes'\n",
    "\n",
    "# get_question(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"This picture describes about inside view of a hall, in this we can find group of people,\\\n",
    "# few are sitting on the chairs, in the background we can see lights.\"\n",
    "# answer = \"chairs\"\n",
    "\n",
    "# get_question(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context = \"In the background I can see few curtains, a door, few candle stands, a fireplace, two lamps\"\n",
    "# answer = \"curtains\"\n",
    "\n",
    "# get_question(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context =  'In this picture I can see 3 stools and the countertops on the floor in front.\\\n",
    "On a countertop, I can see few things. \\\n",
    "In the background, I can see the wall and a cupboard. \\\n",
    "On the top of this picture, I can see the lights on the ceiling. \\\n",
    "On the left side of this picture, I ca can see a white color thing.'\n",
    "# answer = \"left\"\n",
    "\n",
    "# get_question(answer, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U 'spacy[cuda101]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/LIAAD/yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.require_gpu(gpu_id=1)\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 s, sys: 117 ms, total: 1.97 s\n",
      "Wall time: 8.36 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('In this picture few people are sitting on what?', 'chair'),\n",
       " ('What can we see in the background of the picture?', 'light'),\n",
       " ('What is the inside view of?', 'hall'),\n",
       " ('In this picture we can find a group of what?', 'people')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Question types: yes/no, what, where, how many, \n",
    "\n",
    "ignore_list = ['side','background','foreground','picture','image','color','thing','things','group',\\\n",
    "               'view','few','many','some','top','bottom','inside','outside','other','others',\n",
    "               'object','objects','place'\n",
    "              ]\n",
    "\n",
    "\n",
    "def get_qa_pairs(text):\n",
    "    doc = nlp(text)\n",
    "    pos_answers = []\n",
    "    pos_questions = []\n",
    "    pairs = []\n",
    "\n",
    "#     print(doc,\"\\n\")\n",
    "    for word in doc:\n",
    "        if word.tag_ in ['NN','JJ','NNS','CD'] \\\n",
    "        and word.text.lower() not in ignore_list:\n",
    "            pos_answers.append(word.lemma_)\n",
    "    \n",
    "    pos_answers=set(pos_answers) #remove duplicate answers\n",
    "    \n",
    "    for a in pos_answers:\n",
    "        q = get_question(a, text)\n",
    "        if q not in pos_questions: #remove duplicate questions\n",
    "            pairs.append((q,a))\n",
    "            pos_questions.append(q)\n",
    "        \n",
    "    return pairs\n",
    "get_qa_pairs(context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## answers\n",
    "\n",
    "# doc = nlp(context)\n",
    "# pos_answers = []\n",
    "\n",
    "# print(doc,\"\\n\")\n",
    "# for word in doc:\n",
    "#     if word.tag_ in ['NN','JJ','NNS'] \\\n",
    "#     and word.text.lower() not in ['side','background','picture','image','color','thing']:\n",
    "#         pos_answers.append(word.text)\n",
    "#         print(word.text, word.tag_, word.dep_, \", Explanation:\", spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency_answers = []\n",
    "# print(doc,\"\\n\")\n",
    "# for word in doc:\n",
    "#     if word.tag_ in ['NN','JJ','NNS']: \n",
    "#         if word.dep_ in ['nsubj','obj','pobj','dobj']:\n",
    "#             dependency_answers.append(word.text)\n",
    "#             print(word.text, word.dep_, \", Explanation:\", spacy.explain(word.dep_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_questions = []\n",
    "# for i in pos_answers:\n",
    "#     pos_questions.append(get_question(i, context))\n",
    "# for i, j in zip(pos_questions, pos_answers):\n",
    "#     pprint(i)\n",
    "#     pprint(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding questions thru captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE      data_collator.py  prepare_data.py\t\t  utils.py\r\n",
      "README.md    eval.py\t       question_generation.ipynb\r\n",
      "__pycache__  notebooks\t       run_qg.py\r\n",
      "data\t     pipelines.py      trainer.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"../../data/ADE20K-pairs/captions/ade20k_train_captions.jsonl\"\n",
    "val_dir = \"../../data/ADE20K-pairs/captions/ade20k_validation_captions.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_caps = dict()\n",
    "val_caps = dict()\n",
    "\n",
    "\n",
    "with open(train_dir, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = lines\n",
    "    for line in lines:\n",
    "        load = json.loads(line)\n",
    "        train_caps[load['image_id']] = load['caption']\n",
    "        \n",
    "with open(val_dir, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        load = json.loads(line)\n",
    "        val_caps[load['image_id']] = load['caption']\n",
    "        \n",
    "len(train_caps.keys()), len(val_caps.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_qa = dict()\n",
    "# c = 0\n",
    "# for img_id, caption in train_caps.items():\n",
    "#     qa_pairs = get_qa_pairs(caption)\n",
    "#     train_qa[img_id] = qa_pairs\n",
    "#     if not c%50:\n",
    "#         print(c)\n",
    "#         print(caption)\n",
    "#         print(qa_pairs)\n",
    "#     c+=1\n",
    "\n",
    "    \n",
    "# val_qa = dict()\n",
    "# c = 0\n",
    "# for img_id, caption in val_caps.items():\n",
    "#     qa_pairs = get_qa_pairs(caption)\n",
    "#     val_qa[img_id] = qa_pairs\n",
    "#     if not c%50:\n",
    "#         print(c)\n",
    "#         print(caption)\n",
    "#         print(qa_pairs)\n",
    "#     c+=1\n",
    "# train_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dataset-exploration/clip-finetune-ade20k/data/ADE20K-QA\n"
     ]
    }
   ],
   "source": [
    "%cd ../../data/ADE20K-QA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with jsonlines.open('qa_15000-18000.jsonl','w') as f:\n",
    "#     for k, v in train_qa.items():\n",
    "#         f.write({k:v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_qa = dict()\n",
    "val_qa = dict()\n",
    "\n",
    "with jsonlines.open('train_qa.jsonl','r') as f:\n",
    "    for row in f:\n",
    "        train_qa.update(row)        \n",
    "\n",
    "with jsonlines.open('val_qa.jsonl','r') as f:\n",
    "    for row in f:\n",
    "        val_qa.update(row)        \n",
    "len(train_qa.keys()), len(val_qa.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20210"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([int(key.strip(\"ADE_train_\")) for key in train_qa.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(val_qa.keys()).intersection(set(train_qa.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural       nature_landscape     transportation  work_place\r\n",
      "home_or_hotel  shopping_and_dining  unclassified\r\n",
      "industrial     sports_and_leisure   urban\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../ADE20K-pairs/imgs/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25574, 2000)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "train_img_dir = \"../ADE20K-pairs/imgs/training/*/*/*.json\"\n",
    "val_img_dir = \"../ADE20K-pairs/imgs/validation/*/*/*.json\"\n",
    "\n",
    "train_img_paths = glob.glob(train_img_dir)\n",
    "val_img_paths = glob.glob(val_img_dir)\n",
    "\n",
    "len(train_img_paths), len(val_img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.7 s, sys: 1.04 s, total: 11.7 s\n",
      "Wall time: 18.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "train_img_data = {}\n",
    "val_img_data = {}\n",
    "\n",
    "for path in train_img_paths:\n",
    "    with open(path,  encoding=\"utf8\", errors='ignore') as f:\n",
    "        blob = json.load(f)['annotation']\n",
    "        fname = blob['filename'].strip(\".jpg\")\n",
    "#         print(path)\n",
    "        fname = fname.replace(\"frame\",\"train\")\n",
    "        no = int(fname.strip(\"ADE_train_\"))\n",
    "#         print(blob['scene'])\n",
    "        \n",
    "        if no>20210:\n",
    "            continue\n",
    "        objects = []\n",
    "        for o in blob['object']:\n",
    "            objects.append(o['raw_name'])\n",
    "        \n",
    "        train_img_data[fname] = {'scene': blob['scene'],\n",
    "                                 'objects': list(set(objects))\n",
    "                                }\n",
    "        \n",
    "for path in val_img_paths:\n",
    "    with open(path,  encoding=\"utf8\", errors='ignore') as f:\n",
    "        blob = json.load(f)['annotation']\n",
    "        fname = blob['filename'].strip(\".jpg\")\n",
    "#         print(path)\n",
    "        fname = fname.replace(\"frame\",\"val\")\n",
    "        no = int(fname.strip(\"ADE_val_\"))\n",
    "#         print(blob['scene'])\n",
    "\n",
    "        objects = []\n",
    "        for o in blob['object']:\n",
    "            objects.append(o['raw_name'])\n",
    "        \n",
    "        val_img_data[fname] = {'scene': blob['scene'],\n",
    "                                 'objects': list(set(objects))\n",
    "                                }\n",
    "#         print(fname, no)\n",
    "\n",
    "len(train_img_data.keys()), len(val_img_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20210"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([int(key.strip(\"ADE_train_\")) for key in train_img_data.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = dict()\n",
    "val_combined = dict()\n",
    "\n",
    "coarse_scene_types = {'cultural','home or hotel','industrial','nature landscape','shopping and dining',\\\n",
    "                      'sports and leisure','transportation','unclassified','urban','work place'}\n",
    "for k, v in train_qa.items():\n",
    "    ade20k_blob = train_img_data[k] \n",
    "    train_combined[k] = {\n",
    "        'scene-type-coarse':set(ade20k_blob['scene']).intersection(coarse_scene_types),\n",
    "        'scene-type-fine':set(ade20k_blob['scene']) - coarse_scene_types,\n",
    "        'caption': train_caps[k],\n",
    "        'objects':ade20k_blob['objects'],\n",
    "        'qa-pairs': v,\n",
    "    }\n",
    "    \n",
    "\n",
    "for k, v in val_qa.items():\n",
    "    ade20k_blob = val_img_data[k] \n",
    "    val_combined[k] = {\n",
    "        'scene-type-coarse':set(ade20k_blob['scene']).intersection(coarse_scene_types),\n",
    "        'scene-type-fine':set(ade20k_blob['scene']) - coarse_scene_types,\n",
    "        'caption': val_caps[k],\n",
    "        'objects':ade20k_blob['objects'],\n",
    "        'qa-pairs': v,\n",
    "\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scene-type-coarse': {'urban'},\n",
       " 'scene-type-fine': {'highway', 'outdoor'},\n",
       " 'caption': 'In this picture I can see the vehicles on the road. I can see trees on the left side and right side. I can see the light poles. I can see the sky at the top.',\n",
       " 'objects': ['car', 'traffic light', 'road', 'street light', 'sky', 'trees'],\n",
       " 'qa-pairs': [['On what side of the road can I see trees?', 'left'],\n",
       "  ['What do I see on the picture?', 'pole'],\n",
       "  ['What can I see on the road?', 'vehicle'],\n",
       "  ['What kind of poles can I see?', 'light'],\n",
       "  ['What is on the left side of the picture?', 'tree'],\n",
       "  ['What do I see at the top of the picture?', 'sky'],\n",
       "  ['What can I see the vehicles on?', 'road']]}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_combined['ADE_val_00001410']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dataset-exploration/clip-finetune-ade20k/utils/question_generation\n"
     ]
    }
   ],
   "source": [
    "%cd ../../utils/question_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipelines import pipeline\n",
    "\n",
    "qa = pipeline(\"multitask-qa-qg\", model=\"valhalla/t5-base-qa-qg-hl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for qa pass a dict with \"question\" and \"context\"\n",
    "# nlp({\n",
    "#     \"question\": q,\n",
    "#     \"context\": c\n",
    "# })\n",
    "# q = 'What kind of wood are the card boxes on?'\n",
    "# q = 'Along with pipes and doors, what is on the top of the card box?'\n",
    "# c = 'In this image, I see stack of card boxes placed on wooden boards with plastic covers wrapped on them. In the background I see doors, a clock on the wall and few lights and pipes at the top.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = list(nlp.Defaults.stop_words)\n",
    "# len(sw)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "sw.extend(stopwords.words(\"english\"))\n",
    "\n",
    "bad_words = ['', '0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "             'I', 'a', 'above', 'also', 'an', 'and', 'at', 'back', 'background', 'be', 'below',\n",
    "             'behind', 'beside', 'bottom', 'center', 'can', 'corner', 'different','few', 'five', 'foreground','f',\n",
    "             'four', 'front', 'group', 'here', 'image', 'is', 'it', 'in','into','item','items',\n",
    "             'just','kind','left', 'like', 'looks', 'look','many', 'middle', 'number',\n",
    "             'object', 'objects', 'of', 'one', 'or', 'other', 'on','onto',\n",
    "             'photo', 'picture', 'placed', 'photograph', 'right', \n",
    "             's', 'see', 'seems', 'seen', 'seven', \n",
    "             'side', 'six', 'so', 'some', 't', 'taken', 'that', 'the', 'there',\n",
    "             'them', 'this', 'three', 'they','u',\n",
    "             'to', 'top', 'two', 'view', 'we', 'which', 'under','zero']\n",
    "\n",
    "sw.extend(bad_words)\n",
    "\n",
    "bad_questions = ['what is the name of','what can I see in the picture',\n",
    "                 'what do I see in the picture','what can we see in the picture',\n",
    "                 'what do we see in the picture','what can we see in the image',\n",
    "                 'what do we see in the image','what do I see in the image',\n",
    "                 'what do you see?','what is this a picture of'\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "def validate_qa_pairs(caption, pairs):\n",
    "    valid_pairs = []\n",
    "    valid_answers = []\n",
    "    for pair in pairs:\n",
    "        q = pair[0]\n",
    "        \n",
    "        \n",
    "        flag=0\n",
    "        #Filter bad questions\n",
    "        for bq in bad_questions:\n",
    "            if q.lower().startswith(bq):\n",
    "                flag=1\n",
    "        if flag:\n",
    "            continue\n",
    "        \n",
    "        a = pair[1]\n",
    "        predicted_answer = qa({\n",
    "            \"question\":q,\n",
    "            \"context\":caption\n",
    "        })\n",
    "        doc = nlp(predicted_answer)\n",
    "        toks = []\n",
    "        for t in doc:\n",
    "            toks.append(t.lemma_)\n",
    "\n",
    "        \n",
    "        if a in toks:\n",
    "            if 1<len(toks)<3:\n",
    "                predicted_answer = ' '.join([t for t in toks if t not in sw])\n",
    "                if predicted_answer not in valid_answers:\n",
    "#                     if len(predicted_answer.split())>1:\n",
    "#                         print()\n",
    "#                         print(\"Q: \", q, \"Original: \", a, \" | Predicted: \", predicted_answer)\n",
    "                    valid_pairs.append((q,predicted_answer))\n",
    "                    valid_answers.append(predicted_answer)\n",
    "            else:\n",
    "                if a not in valid_answers:\n",
    "                    valid_pairs.append((q,a))\n",
    "                    valid_answers.append(a)\n",
    "    return valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = val_combined['ADE_val_00000115']['caption']\n",
    "# pairs = val_combined['ADE_val_00000115']['qa-pairs']\n",
    "\n",
    "# validate_qa_pairs(c, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating train pairs\n",
      "0\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'bedroom', 'indoor'}, 'caption': 'In this picture I can see the inside view of a room, there is a bed with pillows and blankets on it, there are lamps, there is a photo frame attached to the wall, there is a curtain and a window. Through the window I can see water and the sky.', 'objects': ['curtain', 'window', 'wall', 'bed', 'table lamp', 'pillow', 'headboard', 'cushion', 'ceiling', 'picture', 'night table', 'floor', 'frame'], 'qa-pairs': [('What part of the room is a photo frame attached to?', 'wall'), ('Along with a window, what is on the inside of the room?', 'curtain'), ('Along with blankets, what is on the bed?', 'pillow'), ('In this picture I can see the inside view of what?', 'room'), ('What do I see through the window?', 'sky'), ('Where can I see water and the sky?', 'window'), ('What can I see through the window of a room?', 'water'), ('What is attached to the wall of a room?', 'photo'), ('What is on the inside of the bed?', 'blanket')]}\n",
      "500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'skyscraper', 'outdoor'}, 'caption': 'In this picture, there is a railway track in the bottom right hand corner and back side of this railway track there are many buildings.', 'objects': ['car', 'wall', 'road', 'tunnel', 'building', 'tree', 'platform', 'sign', 'cars', 'railroad track', 'plants', 'buildings', 'skyscraper', 'sky', 'truck', 'station'], 'qa-pairs': [('What is on the back side of the railway track?', 'building'), ('What is in the bottom right corner of the picture?', 'track'), ('On which side of the picture is there a railway track?', 'right'), ('Where is the railway track in the picture?', 'corner'), ('What type of track is in the bottom right corner of the picture?', 'railway')]}\n",
      "1000\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'parking_lot'}, 'caption': 'In this image we can see road, vehicles, poles, wires, plants, board, and houses. In the background there is sky.', 'objects': ['window', 'car', 'bumper', 'road', 'column', 'tractor', 'building', 'flowerpot', 'sign', 'sky'], 'qa-pairs': [('Along with a road, poles and wires, what else can be seen in the background?', 'vehicle'), ('Along with a road, poles, plants, and sky, what else can be seen in the background?', 'board'), ('Along with vehicles, poles, wires and plants, what is seen in the image?', 'road'), ('Along with road, vehicles, poles, wires and houses, what else can be seen in the background?', 'plant')]}\n",
      "1500\n",
      "{'scene-type-coarse': {'shopping and dining'}, 'scene-type-fine': {'indoor', 'shoe_shop'}, 'caption': 'In this image, I see few shoes, a bag, few belts and other objects placed on steel racks and wooden tables. In the background I see a wall and few lights at the top.', 'objects': ['flush mount light', 'wall', 'purse', 'shoe', 'ceiling', 'belt', 'belts', 'floor', 'boot', 'shelves', 'light troffer'], 'qa-pairs': [('Along with shoes, belts and other objects, what do I see in the image?', 'bag'), ('What is in the background of the image?', 'wall'), ('Along with a bag, belts and other objects, what do I see in the image?', 'shoe'), ('What kind of racks are in the image?', 'steel'), ('Along with shoes, a bag and other objects, what is on display in the image?', 'belt'), ('Where are shoes, bags, belts, and other objects placed in the image?', 'rack'), ('What kind of tables are in the image?', 'wooden'), ('What is at the top of the image?', 'light'), ('Where are shoes, bags, belts and other objects placed?', 'table')]}\n",
      "2000\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'In the middle of the image I can see one house. On the right and left side of the image I can see trees. At the bottom of the image I can see the grass. There is a sky on the top of this image.', 'objects': ['window', 'dormer', 'house', 'tree', 'ground', 'door', 'sky', 'trees'], 'qa-pairs': [('What can I see in the middle of the image?', 'house'), ('In what part of the image can I see a house?', 'middle'), ('What is at the bottom of the image?', 'grass'), ('On which side of the image can I see trees?', 'right'), ('On the right and left side of the image, what can I see?', 'tree'), ('What is on the top of the image?', 'sky'), ('How many houses are visible in the middle of the image?', 'one')]}\n",
      "2500\n",
      "{'scene-type-coarse': {'cultural'}, 'scene-type-fine': {'sacristy', 'indoor'}, 'caption': 'In this image there are sculptures, wall, candles, cross, pillars and some other objects. At the bottom there is floor.', 'objects': ['window', 'wall', 'steps', 'candelabra', 'pantheon', 'altar', 'cross', 'floor', 'door', 'sculpture'], 'qa-pairs': [('Along with candles, a cross and pillars, what else is on display in the image?', 'sculpture'), ('What is at the bottom of the image?', 'floor'), ('Along with sculptures, wall, cross, pillars and other objects, what is on display in the image?', 'candle')]}\n",
      "3000\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'kitchen', 'indoor'}, 'caption': 'This is an inside view of a room. On the right side there is a table and few chairs around it. In the background, I can see the table cabinets. On the table few bowls, a machine and some other objects are placed. At the top there are cupboards and few plants are hanging to the cupboards. In the top right-hand corner there is a lantern.', 'objects': ['wall', 'dishwasher', 'stove', 'pendant lamp', 'wok', 'table', 'coffee maker', 'box', 'saucepan', 'cabinet', 'oven', 'glass', 'sink', 'microwave', 'range', 'floor', 'frying pan', 'chair', 'work surface', 'window', 'chest of drawers', 'tablecloth', 'ceiling', 'door', 'display', 'faucet', 'button panel', 'extractor hood'], 'qa-pairs': [('What is on the right side of the table?', 'chair'), ('What is hanging to the cupboards?', 'plant'), ('What is at the top of the table?', 'cupboard'), ('In what part of the room is a lantern located?', 'corner'), ('On which side of the room is a table and chairs?', 'right'), ('On which side of the room is a lantern located?', 'hand'), ('What is in the top right corner of the room?', 'lantern'), ('What is placed on the table?', 'bowl'), ('What is on the right side of the room?', 'table'), ('This is an inside view of what?', 'room')]}\n",
      "3500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'ruin', 'outdoor'}, 'caption': 'In this image we can see some old and damaged buildings and here we can see some rocks,steps and some big towers and the towers are on some cement blocks and in the front there is a grass area with grass and some other rocks also and in the background we can see some trees and some other damaged sculptures and in the background we can see the sky.', 'objects': ['steps', 'ruins', 'column', 'building', 'tree', 'railing', 'ground', 'fence', 'buildings', 'sky', 'trees'], 'qa-pairs': [('What type of buildings are in the image?', 'old'), ('Along with rocks and towers, what else can be seen in the image?', 'step'), ('What can be seen in this image?', 'building'), ('What type of cement are the towers on?', 'block'), ('What is on some cement blocks?', 'tower'), ('What kind of area is on the front of the buildings?', 'grass'), ('Along with steps and towers, what else can be seen in the image?', 'rock'), ('What type of blocks are the towers on?', 'cement'), ('What kind of towers are in the image?', 'big tower'), ('What kind of buildings are in the image?', 'damaged'), ('In what part of the image is there a grass area?', 'front'), ('What is in the front of the buildings?', 'area')]}\n",
      "4000\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'ski_lodge'}, 'caption': 'In this image, we can see people wearing coats, some are on the vehicles and they are wearing helmets and glasses. We can see trees, lights, boards with text, poles, railings and there is a building. At the top, there is sky. At the bottom, there is snow.', 'objects': ['sled', 'building', 'snow', 'person', 'sky', 'trees'], 'qa-pairs': [('What is at the bottom of the image?', 'snow'), ('On what type of vehicle are people wearing coats?', 'vehicle'), ('What do people wear in the image?', 'coat'), ('Along with helmets, what else are people wearing in the image?', 'glass'), ('What is on the boards in the image?', 'text'), ('What type of tree is visible in the image?', 'tree'), ('Who wears coats?', 'people'), ('Along with glasses, what are people wearing in the image?', 'helmet')]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'indoor', 'hotel_room'}, 'caption': 'In this picture I can see the sofa, bed, mirror on the right side. I can see the wooden table, paper, plant vase in the foreground. I can see the curtain, wooden object, electronic device on the left side. I can see the hanging light.', 'objects': ['window', 'magazine', 'wall', 'bed', 'vase', 'dresser', 'curtain', 'skirt', 'back', 'flowers', 'ceiling', 'coffee table', 'floor', 'sofa', 'seat base', 'seat cushion', 'mirror', 'arm', 'semi-flush mount light', 'radiator'], 'qa-pairs': [('What is on the right side of the picture?', 'sofa'), ('On which side of the picture is the sofa?', 'right'), ('What type of device is on the left side of the picture?', 'electronic'), ('What type of table is in the foreground?', 'wooden'), ('What type of vase is in the foreground?', 'plant vase')]}\n",
      "5000\n",
      "{'scene-type-coarse': {'work place'}, 'scene-type-fine': {'parking_garage indoor', 'indoor'}, 'caption': 'In this image there are a few cars in the basement parking, beside the cars there are pillars and there are sign boards on the walls. At the top of the image there are lamps on the roof. On the surface there are markings.\\xa0', 'objects': ['car', 'wall', 'license plate', 'column', 'ceiling', 'sign', 'floor', 'wheel', 'taillight'], 'qa-pairs': [('What part of the building has markings on it?', 'surface'), ('What is on the roof of the building?', 'lamp'), ('Lamps are on what part of the building?', 'roof'), ('What is on the surface of the building?', 'marking'), ('What is beside the cars in the basement parking area?', 'pillar'), ('What is on the walls of the basement parking lot?', 'sign board'), ('Where are a few cars in the basement?', 'parking'), ('On what part of the basement is there sign boards?', 'wall'), ('What is in the basement parking?', 'car'), ('In what part of the building are a few cars stored?', 'basement parking')]}\n",
      "5500\n",
      "{'scene-type-coarse': {'home or hotel', 'work place'}, 'scene-type-fine': {'staircase', 'indoor'}, 'caption': 'The image is taken inside a building. In the middle of the picture we can see staircase. On the left we can see wall, frame and hand railing. On the right we can see frame, wall and some wooden objects.', 'objects': ['wall', 'staircase', 'ceiling', 'picture', 'handrail', 'tread', 'door', 'step', 'riser'], 'qa-pairs': [('What is in the middle of the picture?', 'staircase'), ('What kind of railing is on the left of the picture?', 'hand railing'), ('What type of object is on the right of the picture?', 'wooden'), ('In what part of the picture is a staircase visible?', ''), ('What is the image taken inside?', 'building')]}\n",
      "6000\n",
      "{'scene-type-coarse': {'nature landscape'}, 'scene-type-fine': {'marsh', 'outdoor'}, 'caption': 'In this picture we see a lake surrounded by grass and many trees and at the top we can see that the sky is bright.', 'objects': ['trees', 'undergrowth', 'bushes', 'sky', 'water'], 'qa-pairs': [('What is bright at the top of the lake?', 'sky'), ('What color is the sky in this picture?', 'bright'), ('What type of vegetation surrounds the lake?', 'grass')]}\n",
      "6500\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'In this image I can see the grass, many trees, buildings and the towers. In the background I can see the clouds and the sky.', 'objects': ['sky', 'field', 'building', 'tree'], 'qa-pairs': [('What do I see in the background of the image?', 'sky'), ('Along with many trees, buildings and towers, what is visible in the image?', 'grass'), ('What is in the background of the image?', 'cloud')]}\n",
      "7000\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'bedroom', 'indoor'}, 'caption': 'As we can see in the image there is white color wall, bed, pillow, two curtains and window.', 'objects': ['window', 'curtain', 'wall', 'bed', 'pillow', 'floor'], 'qa-pairs': [('Along with a bed, pillow and two curtains, what else is present in the room?', 'window'), ('How many curtains are in the room?', 'two'), ('Along with the bed, curtains and window, what else is present in the room?', 'pillow'), ('Along with the bed, pillow, and window, what else is present in the room?', 'curtain'), ('What color is the wall, bed, pillow, two curtains and window?', 'white'), ('What part of the house has a white color?', 'wall'), ('Along with a pillow, two curtains and window, what else is present in the room?', 'bed')]}\n",
      "7500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'street'}, 'caption': 'In this picture we can see double decker buses and few vehicles on the road. We can see buildings with windows. We can see trees and light poles. At the bottom of the image we can see roads on the road. On the right side of the picture boards are visible.\\xa0', 'objects': ['window', 'car', 'road', 'windshield', 'rim', 'building', 'tree', 'pole', 'sidewalk', 'person', 'van', 'cars', 'door', 'wheel', 'sky', 'advertisement', 'bus'], 'qa-pairs': [('What type of bus is seen in the picture?', 'decker'), ('Along with light poles, what is visible in the picture?', 'tree'), ('How many decker buses are in the picture?', 'double'), ('What do we see at the bottom of the picture?', 'road'), ('What kind of poles are visible in the picture?', 'light pole'), ('What is visible on the right side of the picture?', 'board'), ('What type of vehicle is seen in the picture?', 'bus'), ('What can we see with windows in the picture?', 'building'), ('On which side of the picture are boards visible?', 'right')]}\n",
      "8000\n",
      "{'scene-type-coarse': {'sports and leisure'}, 'scene-type-fine': {'lido_deck indoor', 'indoor'}, 'caption': 'On the right side of this image there is a pool and there are few metal stands. In the background there are chairs and few people are standing and also I can see the pillars. At the top of the image I can see the roof with the metal rods.', 'objects': ['wall', 'steps', 'umbrella', 'pool', 'ceiling', 'handrail', 'sign', 'floor', 'person'], 'qa-pairs': [('What kind of stands are on the right side of the image?', 'metal'), ('What is on the right side of the image?', 'pool'), ('What is on the top of the image?', 'roof'), ('What is on the roof of the pool?', 'metal rod'), ('On which side of the image is there a pool?', 'right'), ('What is in the background of the image?', 'chair')]}\n",
      "8500\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'living_room', 'indoor'}, 'caption': 'In this image, in the middle there are chairs, table, sofas, pillows, musical instrument, photo frames, lamp, cupboards, some objects, wall. This image is clicked inside a room.', 'objects': ['wall', 'column', 'cushion', 'picture', 'floor lamp', 'light source', 'table', 'cabinet', 'seat base', 'manchette', 'highlight', 'inside arm', 'h-stretcher', 'stile', 'outside arm', 'bowl', 'seat', 'top', 'flower', 'sofa', 'floor', 'chair', 'shade', 'rail', 'chest of drawers', 'back', 'ceiling', 'seat cushion', 'frame', 'arm', 'semi-flush mount light', 'leg', 'table lamp', 'door frame', 'arm support', 'apron', 'mirror', 'piano', 'back pillow', 'base'], 'qa-pairs': [('What is the image clicked inside?', 'room'), ('What type of instrument is in the middle of the image?', 'musical'), ('In what part of the image are chairs, table, sofas, pillows, musical instrument, photo frames, lamp, cupboards and wall?', 'middle')]}\n",
      "9000\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'At the bottom of the image there are trees. Behind those trees there are buildings and walls. At the top of the image there is sky.', 'objects': ['ruins', 'acropolis', 'hill', 'sky', 'trees'], 'qa-pairs': [('What is at the top of the image?', 'sky'), ('What part of the image is behind the trees?', 'wall'), ('Behind the trees are walls and what else?', 'building'), ('What is at the bottom of the image?', 'tree')]}\n",
      "9500\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'indoor', 'dining_room'}, 'caption': 'This image consists of a table along with the chairs. At the top, there is a roof and a lamp hangs to it. On the left, there is a mirror. On the right, we can see a window and an air conditioner.', 'objects': ['window', 'wall', 'pendant lamp', 'air conditioning', 'table', 'floor', 'step', 'coffee cup', 'chair', 'cabinet', 'plant'], 'qa-pairs': [('Along with an air conditioner, what is visible on the right?', 'window'), ('What hangs to the top of the table?', 'lamp'), ('What is at the top of the table?', 'roof'), ('What is on the right of the window?', 'air conditioner'), ('Along with chairs, what is in the image?', 'table'), ('What is on the left of the image?', 'mirror'), ('Along with a table, what else is in the image?', 'chair'), ('On which side of the image is there a window and an air conditioner?', 'right')]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'indoor', 'dining_room'}, 'caption': 'There are chairs around the table and a bowl on the table in the foreground area of the image, there are windows, curtains, lamp frame, greenery and the sky in the background and a chandelier at the top side.', 'objects': ['window', 'curtain', 'wall', 'bowl', 'table lamp', 'chandelier', 'ceiling', 'picture', 'armchair', 'switch', 'table', 'floor', 'shelves', 'chair', 'cabinet'], 'qa-pairs': [('What is on the top of the image?', 'chandelier'), ('What is in the foreground area of the image?', 'bowl'), ('What is on the frame of the image?', 'lamp frame')]}\n",
      "10500\n",
      "{'scene-type-coarse': {'work place'}, 'scene-type-fine': {'indoor', 'maternity_ward'}, 'caption': 'There are cribs at the bottom of this image. There is one person standing and wearing a white color dress. We can see some objects and a window on the left side of this image. There is a wall in the background.', 'objects': ['window', 'wall', 'cradle', 'scales', 'person', 'floor'], 'qa-pairs': [('How many people are in the image?', 'one'), ('What is at the bottom of the image?', 'crib'), ('What color dress is the person in the image wearing?', 'white'), ('What is in the background of the image?', 'wall'), ('Who is standing and wearing a white color dress?', 'person'), ('What is on the left side of the image?', 'window')]}\n",
      "11000\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'market outdoor'}, 'caption': 'In this image, we can see a cart stall. In this stall, there are so many hats and scarves are placed. Background we can see buildings, walls, windows, flag, banners, trees and sky. On the left side of the image, we can see a group of people are on the path.', 'objects': ['window', 'hats', 'mirror', 'building', 'tree', 'shawl', 'mast', 'sidewalk', 'person', 'wheel', 'sign', 'street light', 'hat', 'sky', 'flag', 'stand'], 'qa-pairs': [('What is on the background of the cart stall?', 'banner'), ('What type of stall is seen in the image?', 'cart stall'), ('On the left side of the image, what is on the path?', 'people'), ('Along with scarves, what is placed on the cart stall?', 'hat'), ('On the left side of the image, a group of people are on what?', 'path'), ('Along with hats, what is placed in the cart stall?', 'scarf')]}\n",
      "11500\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'In this picture I can see number of sitting chair on the left side I can see a metal grill fence on the right side. I can see hanging DJ lights at the top. I can see the doors.', 'objects': ['curtain', 'seats', 'wall', 'staircase', 'ceiling', 'railing', 'ceiling spotlight', 'stage', 'floor', 'sign', 'door', 'flag'], 'qa-pairs': [('What is hanging at the top of the fence?', 'dj light'), ('What is the number of sitting chairs on the left side?', 'number'), ('What kind of fence is on the right side of the car?', 'metal grill'), ('What is on the left side of the picture?', 'sit chair'), ('What kind of fence is on the right side of the grill?', 'metal'), ('On which side is a metal grill fence visible?', '')]}\n",
      "12000\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'The image is taken inside the house. On the left we can see many objects like bags, cpus, table and other objects. In the middle we can see wall, switches and other objects. At the top it is ceiling. In the center of the background towards right we can see window, wall and other objects. At the bottom there is floor.', 'objects': ['wall', 'monitor', 'computer case', 'sprinkler', 'screen', 'picture', 'ceiling', 'switch', 'table', 'floor', 'box', 'door', 'shelves', 'water cooler', 'trash can', 'paper', 'ceiling recessed light', 'computer', 'outlet'], 'qa-pairs': [('In what part of the image is wall, switches and other objects visible?', 'middle'), ('What is at the top of the image?', 'ceiling'), ('In what part of the background is a window seen?', 'center'), ('What is the image taken inside?', 'house'), ('What is in the center of the background?', 'window'), ('What is at the bottom of the image?', 'floor')]}\n",
      "12500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'skyscraper', 'outdoor'}, 'caption': 'This is an outside view. In this image, I can see many buildings. At the bottom there are few poles and trees. At the top of the image I can see the sky.', 'objects': ['skyscraper', 'road', 'building', 'river water', 'buildings', 'mountain', 'sky', 'brand name'], 'qa-pairs': [('What is at the bottom of the image?', 'tree'), ('What do I see at the top of the image?', 'sky')]}\n",
      "13000\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'greenhouse outdoor', 'outdoor'}, 'caption': 'In the foreground of this image, there are plants on the grass and in the shelter. In the background, there are trees.', 'objects': ['grass', 'flowerpot', 'greenhouse', 'door', 'plants', 'plant', 'trees'], 'qa-pairs': [('What is in the background of the image?', 'tree'), ('What is in the foreground of the image?', 'plant'), ('In the foreground of the image, what are plants on?', 'grass')]}\n",
      "13500\n",
      "{'scene-type-coarse': {'industrial'}, 'scene-type-fine': {'junkyard', 'outdoor'}, 'caption': 'In the center of the image we can see old rusted car. At the bottom of the image we can see plants and grass. In the background we can see rusted cars and trees.', 'objects': ['ground', 'car scrapping', 'bushes'], 'qa-pairs': [('At the bottom of the image we can see plants and what else?', 'grass'), ('In what part of the image can we see a rusted car?', 'center'), ('What type of car is in the center of the image?', 'rusted'), ('What is in the background of the image?', 'tree'), ('What is in the center of the image?', 'car')]}\n",
      "14000\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'In this image we can see trees, water, grass and sky.', 'objects': ['grass', 'river water', 'tree', 'bridge', 'sky', 'trees'], 'qa-pairs': [('Along with trees, water, grass and what else can be seen in the image?', 'sky'), ('Along with water, grass and sky, what can be seen in the image?', 'tree'), ('Along with trees, grass and sky, what else can be seen in the image?', 'water'), ('Along with trees, water, and sky, what else can be seen in the image?', 'grass')]}\n",
      "14500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'street'}, 'caption': 'In this image we can see road, plants, trees, board, frames, doors, windows, pole, lights, houses, and few objects. In the background there is sky.', 'objects': ['window', 'road', 'building', 'tree', 'double door', 'sidewalk', 'sign', 'trash can', 'sky'], 'qa-pairs': [('Along with plants, trees, board, frames, doors, windows, pole, lights, houses, and sky, what can be seen in th', 'road')]}\n",
      "15000\n",
      "{'scene-type-coarse': {'sports and leisure'}, 'scene-type-fine': {'park', 'outdoor'}, 'caption': 'In this image there is a rock structure. At the bottom of the image there are dried leaves on the surface. There are stones. There are plants, trees. At the top of the image there is sky.', 'objects': ['tree', 'grass', 'ground', 'sky', 'plant', 'sculpture'], 'qa-pairs': [('What part of the rock structure is dried leaves on?', 'surface'), ('What structure is at the bottom of the image?', 'rock'), ('What is at the top of the image?', 'sky'), ('What is dried on the surface of the rock structure?', 'leave')]}\n",
      "15500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'cloister outdoor', 'outdoor'}, 'caption': 'In this image we can see grass, plants, tree, pillars, building, and few objects.', 'objects': ['window', 'arcades', 'steps', 'building', 'tree', 'grass', 'hedge', 'flowerpot', 'pendant lamp', 'roof', 'door', 'sky', 'plant'], 'qa-pairs': [('Along with plants, trees, pillars, and building, what can be seen in the image?', 'grass'), ('What is an example of a building?', 'pillar'), ('In this image we can see grass, plants, pillars, building, and what other object?', 'tree'), ('In this image we can see grass, tree, pillars, building, and what other object?', 'plant')]}\n",
      "16000\n",
      "{'scene-type-coarse': {'nature landscape'}, 'scene-type-fine': {'underwater coral_reef', 'outdoor'}, 'caption': 'In the image, inside the water there are corals and also there are fishes.', 'objects': ['plant', 'rocks', 'fish', 'water'], 'qa-pairs': [('Corals and fishes are found inside what?', 'water'), ('Inside the water are corals and what else?', 'fish'), ('Along with fishes, what is found in the water?', 'coral')]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16500\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'living_room', 'indoor'}, 'caption': 'In this image on the left side, we can see the sofas. And we can see the lamp and cupboard. And we can see the frames on the wall. And on the right, we can see the monitor. And we can see some objects which are arranged in the wooden cupboard.', 'objects': ['wall', 'books', 'cushion', 'picture', 'shelf', 'pane', 'back pillow', 'oven', 'side', 'television', 'top', 'sofa', 'floor', 'ottoman', 'handle', 'work surface', 'desk lamp', 'ceiling', 'loudspeaker', 'door', 'seat cushion', 'frame', 'arm', 'leg', 'music system', 'front', 'shelves', 'cabinet', 'ceiling recessed light'], 'qa-pairs': [('On what part of the house can we see the frames?', 'wall'), ('In the image on the left, we can see the lamp and what else?', 'cupboard'), ('Along with the cupboard, what is visible in the image on the left?', 'lamp'), ('On which side of the image is the monitor?', 'right'), ('What kind of cupboard is on the right?', 'wooden'), ('What is on the right side of the image?', 'monitor'), ('What is on the wall?', 'frame')]}\n",
      "17000\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'In this picture we can see buildings, in front there is road with vehicles, beside there are trees, top there is blue sky.', 'objects': ['car', 'stones', 'monument', 'road', 'arcades', 'steps', 'building', 'tree', 'chimney', 'sidewalk', 'person', 'street light', 'door', 'terrace', 'sky', 'plant'], 'qa-pairs': [('In front of buildings is a road with what?', 'vehicle'), ('What color is the sky?', 'blue'), ('What is in front of the buildings?', 'road'), ('What is next to the road?', 'tree'), ('What is on the top of the picture?', 'blue sky'), ('In what direction is there a road with vehicles?', 'front')]}\n",
      "17500\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'indoor', 'bathroom'}, 'caption': 'In this image I can see wash basin with tap and I can also see cupboards, drawers, mirror, lights, glass shelf, holder with toilet paper roll, western toilet, floor, marble tiles, wall, handle with some object and in the background I can see windows, glasses, some item and trees.', 'objects': ['flush mount light', 'wall', 'heater', 'shelf', 'light source', 'toilet', 'toilet paper', 'sconce', 'sink', 'countertop', 'floor', 'tap', 'towel', 'shade', 'handle', 'window', 'ceiling', 'door', 'drawer', 'arm', 'faucet', 'curtain', 'knob', 'mirror', 'towel ring', 'cabinet', 'outlet'], 'qa-pairs': [('What type of tiles are in the image?', 'marble tile'), ('On what part of the image can I see a handle with some object?', 'wall'), ('What is a holder for toilet paper?', 'roll'), ('What type of toilet is on the image?', 'western toilet'), ('Along with drawers, mirror, lights and western toilet, what else can be seen in the image?', 'cupboard'), ('What is a toilet paper roll in the image?', 'holder'), ('What kind of roll does western toilet have?', 'paper'), ('What kind of shelf is in the background?', 'glass shelf'), ('What is in the background of the image?', 'tree'), ('In the background I can see windows, glasses, trees and what else?', ''), ('Along with cupboards and mirrors, what else can be seen in the image?', 'drawer'), ('What is in the wash basin?', 'tap')]}\n",
      "18000\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'indoor', 'dorm_room'}, 'caption': 'In this image we can see the bed, chair and table. And on the right, we can see some television monitor, clock, lights. And we can see some books, mirrors, dolls and objects arranged on shelves. And we can see the curtain. And we can see the bags on the table.', 'objects': ['wall', 'bed', 'footboard', 'shelf', 'clock', 'table', 'mattress', 'blanket', 'book', 'side', 'bedpost', 'television', 'stile', 'suitcase', 'toy', 'seat', 'top', 'headboard', 'floor', 'lock', 'chair', 'notebooks', 'rail', 'window', 'desk', 'desk lamp', 'purse', 'back', 'digital clock', 'ladder', 'drawer', 'side rail', 'bottle', 'leg', 'curtain', 'notebook', 'knob', 'pillow', 'stretcher', 'folder', 'front', 'jacket', 'keys', 'shelves', 'apron', 'mirror'], 'qa-pairs': [('Where are books, mirrors, dolls and objects arranged?', 'shelf'), ('Along with the bed and chair, what is on the right?', 'table'), ('What is on the table?', 'bag'), ('Along with books and dolls, what is on the right of the image?', 'mirror'), ('Along with the chair and table, what is on the right?', 'bed'), ('On what side of the image can we see a television, clock, and lights?', 'right')]}\n",
      "18500\n",
      "{'scene-type-coarse': {'work place'}, 'scene-type-fine': {'indoor', 'lobby'}, 'caption': 'In this image, we can see chairs, couches, tables, flower vase, floor, floor mat and show pieces. In the background we can see mirrors, frame, walls, stairs, railings, musical instrument, stool and few objects. Top of the image, we can see the ceiling and chandeliers.', 'objects': ['vase', 'wall', 'grand piano', 'cushion', 'coffee table', 'picture', 'flowerpot', 'table', 'sofa', 'floor', 'chair', 'plant', 'sculpture', 'side table', 'flowers', 'ceiling', 'armchair', 'railing', 'chandelier', 'console table', 'mirror', 'stool', 'base'], 'qa-pairs': [('In the background of the image, we can see a flower what?', 'vase'), ('In the background of the image, what is a mat?', 'floor mat'), ('What is on the top of the image?', 'chandelier'), ('Along with chandeliers, what is seen on the top of the image?', 'ceiling'), ('What type of vase can be seen in the image?', 'flower vase'), ('What type of instrument can be seen in the background?', 'musical')]}\n",
      "19000\n",
      "{'scene-type-coarse': {'unclassified'}, 'scene-type-fine': set(), 'caption': 'In the image there is a building and in front of the building there is a garden, it is a blur picture.', 'objects': ['window', 'wall', 'building', 'grass', 'door', 'balcony', 'sky'], 'qa-pairs': [('In what part of the building is there a garden?', 'front'), ('What is the picture of the building and the garden in front of it?', 'blur'), ('What is in front of the building?', 'garden'), ('What is in the image?', 'building')]}\n",
      "19500\n",
      "{'scene-type-coarse': {'industrial'}, 'scene-type-fine': {'outdoor', 'industrial_park'}, 'caption': 'In this image we can see a shed with windows and doors. There is a fencing. To the left side of the image there are cars. To the both side of the image there are trees. At the top of the image there is sky.', 'objects': ['window', 'car', 'road', 'building', 'tree', 'ground', 'fence', 'sky'], 'qa-pairs': [('Along with doors, what does the shed have?', 'window'), ('What is at the top of the image?', 'sky'), ('What is on the other side of the image?', 'tree'), ('What is seen in the image with windows and doors?', 'shed'), ('Along with windows, what does the shed have?', 'door')]}\n",
      "20000\n",
      "{'scene-type-coarse': {'nature landscape'}, 'scene-type-fine': {'outdoor', 'sinkhole'}, 'caption': 'In this image we can see a green color water pond with water and on the top of the pond there is a man is flying with the help of a tree root and we can also see some plants are around the water pond and people are standing and sitting around the water pond and we can also see some mountains and on the mountains there are some trees and plants and we can also see a blue color object also.', 'objects': ['trees', 'tree', 'dirt track', 'ground', 'person', 'bushes', 'rope', 'water'], 'qa-pairs': [('What color is the water pond in this image?', 'green'), ('What kind of root is on the top of the water pond?', 'tree root'), ('On what mountain is there a blue color object?', 'mountain'), ('What is around the water pond?', 'plant'), ('What does the green color water pond have?', 'water'), ('Who is flying on the top of the water pond?', 'man'), ('Who is standing and sitting around the water pond?', 'people')]}\n",
      "Validating val pairs\n",
      "0\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'highway'}, 'caption': 'In this picture I can see the vehicles on the road. I can see trees on the left side and right side. I can see the light poles. I can see the sky at the top.', 'objects': ['car', 'traffic light', 'road', 'street light', 'sky', 'trees'], 'qa-pairs': [('What can I see on the road?', 'vehicle'), ('What kind of poles can I see?', 'light pole'), ('What is on the left side of the picture?', 'tree'), ('What do I see at the top of the picture?', 'sky'), ('What can I see the vehicles on?', 'road')]}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'outdoor', 'street'}, 'caption': 'In this picture we can see houses beside the road, where we can see shops, there is text on top of shop, there is black gate, there are glass windows to the houses.', 'objects': ['window', 'alarm', 'road', 'shop window', 'building', 'mailbox', 'sidewalk', 'roof', 'door', 'lock', 'handle', 'brand name'], 'qa-pairs': [('What is on top of the shop?', 'text'), ('What color is the gate in the picture?', 'black'), ('What kind of windows are on the houses?', 'glass'), ('What can we see beside the road?', 'house'), ('Where can we see shops?', 'road')]}\n",
      "1000\n",
      "{'scene-type-coarse': {'urban'}, 'scene-type-fine': {'yard', 'outdoor'}, 'caption': 'In the picture I can see flower plants and the grass. In the background I can see trees and the sky.', 'objects': ['sky', 'grass', 'plants', 'plant'], 'qa-pairs': [('Along with the grass, what type of plants can be seen in the picture?', 'flower plant'), ('Along with flower plants, what is visible in the picture?', 'grass'), ('What do I see in the picture?', 'plant'), ('What type of tree is in the background?', 'tree'), ('In the background I can see trees and what else?', 'sky')]}\n",
      "1500\n",
      "{'scene-type-coarse': {'home or hotel'}, 'scene-type-fine': {'indoor', 'closet'}, 'caption': 'This image is taken indoors. In the background there is a wall. At the bottom of the image there is a floor. In the middle of the image there are many clothes in the closet and there are a few cupboards.', 'objects': ['wall', 'chest of drawers', 'sweater', 'wardrobe', 'knob', 'shoe', 't-shirt', 'ceiling', 'shelf', 'jacket', 'trousers', 'shirt', 'cap', 'floor', 'box', 'drawer'], 'qa-pairs': [('What is in the background of the image?', 'wall'), ('In what part of the image are there many clothes in the closet?', 'middle'), ('In the middle of the image there are many clothes in what?', 'closet'), ('In the middle of the image, what is in the closet?', 'clothe'), ('What is at the bottom of the image?', 'floor')]}\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(\"Validating train pairs\")\n",
    "for k, v in train_combined.items():\n",
    "    c = v['caption']\n",
    "    pairs = v['qa-pairs']\n",
    "    valid_pairs = validate_qa_pairs(c, pairs)\n",
    "    train_combined[k]['qa-pairs'] = valid_pairs\n",
    "    if not i%500:\n",
    "        print(i)\n",
    "        print(train_combined[k])\n",
    "    i+=1\n",
    "        \n",
    "i=0  \n",
    "print(\"Validating val pairs\")\n",
    "for k, v in val_combined.items():\n",
    "    c = v['caption']\n",
    "    pairs = v['qa-pairs']\n",
    "    valid_pairs = validate_qa_pairs(c, pairs)\n",
    "    val_combined[k]['qa-pairs'] = valid_pairs\n",
    "    if not i%500:\n",
    "        print(i)\n",
    "        print(val_combined[k])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in train_combined.items():\n",
    "#     train_combined[k]['scene-type-coarse'] = train_combined[k]['scene-type-coarse'].pop()\n",
    "#     train_combined[k]['scene-type-fine'] = list(train_combined[k]['scene-type-fine'])\n",
    "\n",
    "# for k, v in val_combined.items():\n",
    "#     val_combined[k]['scene-type-coarse'] = val_combined[k]['scene-type-coarse'].pop()\n",
    "#     val_combined[k]['scene-type-fine'] = list(val_combined[k]['scene-type-fine'])\n",
    "\n",
    "# train_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dataset-exploration/clip-finetune-ade20k/data\n"
     ]
    }
   ],
   "source": [
    "%cd ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with jsonlines.open('ade20k-train-combined.jsonl','w') as f:\n",
    "#     for k, v in train_combined.items():\n",
    "#         f.write({k:v})\n",
    "        \n",
    "# with jsonlines.open('ade20k-val-combined.jsonl','w') as f:\n",
    "#     for k, v in val_combined.items():\n",
    "#         f.write({k:v})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "904"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections \n",
    "import nltk  # $ pip install nltk\n",
    "from nltk.corpus import cmudict  # >>> nltk.download('cmudict')\n",
    "\n",
    "all_objects = []\n",
    "\n",
    "for k, v in train_combined.items():\n",
    "    all_objects.extend(v['objects'])\n",
    "    \n",
    "# len(all_objects), len(set(all_objects))\n",
    "\n",
    "acceptable = []\n",
    "for k,v in collections.Counter(all_objects).most_common():\n",
    "#     print(k)\n",
    "#     print(v)\n",
    "#     break\n",
    "    if v>=10:\n",
    "        acceptable.append(k)\n",
    "        \n",
    "all_objects = set(acceptable)\n",
    "len(all_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def starts_with_vowel_sound(word, pronunciations=cmudict.dict()):\n",
    "    for syllables in pronunciations.get(word, []):\n",
    "        return syllables[0][-1].isdigit()  # use only the first one\n",
    "\n",
    "x = []\n",
    "\n",
    "for obj in all_objects:\n",
    "    for i, doc in enumerate(nlp(obj)):\n",
    "        if i==0:\n",
    "            if doc.tag_ != 'NNS':\n",
    "                article = \"an \"+ obj if starts_with_vowel_sound(doc.text) else \"a \" + obj\n",
    "            else:\n",
    "                article = None\n",
    "            x.append((obj, article))\n",
    "\n",
    "all_objects = dict()\n",
    "\n",
    "for i in x:\n",
    "    all_objects[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_templates_singular = [\"Is there %s?\", \"Is there %s in this image?\"]\n",
    "question_templates_plural = [\"Are there %s?\", \"Are there %s in this photo?\", \"Are there %s in this image?\"]\n",
    "question_templates_generic = [\"Do you see %s?\", \"Can you see %s?\", \"Do you see %s in this picture?\",\n",
    "                              \"Can you see %s in this photo?\", \"Does this picture have %s?\",\n",
    "                              \"In this image, can you see %s?\", \"Can we see %s in this picture?\",\n",
    "                              \"Does this image consist of %s?\", \"Does this photo have %s?\",\n",
    "                             ]\n",
    "\n",
    "question_templates_singular.extend(question_templates_generic)\n",
    "question_templates_plural.extend(question_templates_generic)\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def create_binary_questions(objects):\n",
    "    objects = set(objects)\n",
    "    all_objects_ = all_objects.keys()\n",
    "    \n",
    "    yes = objects.intersection(all_objects_)\n",
    "    no = all_objects_ - objects\n",
    "    \n",
    "    question_pairs = []\n",
    "    \n",
    "    for item in yes:\n",
    "        if all_objects[item]:\n",
    "            string = random.choice(question_templates_singular) % all_objects[item]\n",
    "            question_pairs.append((string, \"yes\"))\n",
    "        else:\n",
    "            string = random.choice(question_templates_plural) % item\n",
    "            question_pairs.append((string, \"yes\"))\n",
    "            \n",
    "    no_sample_size = max(2, len(yes) + random.choice(range(-3,3)))\n",
    "    no = random.sample(list(no), no_sample_size)\n",
    "    \n",
    "    for item in no:\n",
    "        if all_objects[item]:\n",
    "            string = random.choice(question_templates_singular) % all_objects[item]\n",
    "            question_pairs.append((string, \"no\"))\n",
    "        else:\n",
    "            string = random.choice(question_templates_plural) % item\n",
    "            question_pairs.append((string, \"no\"))\n",
    "            \n",
    "    return question_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in train_combined.items():\n",
    "    train_combined[k]['qa-binary'] = create_binary_questions(train_combined[k]['objects'])\n",
    "\n",
    "for k, v in val_combined.items():\n",
    "    val_combined[k]['qa-binary'] = create_binary_questions(val_combined[k]['objects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('ade20k-train-combined.jsonl','w') as f:\n",
    "    for k, v in train_combined.items():\n",
    "        f.write({k:v})\n",
    "        \n",
    "with jsonlines.open('ade20k-val-combined.jsonl','w') as f:\n",
    "    for k, v in val_combined.items():\n",
    "        f.write({k:v})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating vqa train and val sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonlines\n",
    "train = dict()\n",
    "val = dict()\n",
    "\n",
    "basepath = \"../data/\"\n",
    "\n",
    "with jsonlines.open(basepath+'ade20k-train-combined.jsonl','r') as f:\n",
    "    for row in f:\n",
    "        train.update(row)        \n",
    "\n",
    "with jsonlines.open(basepath+'ade20k-val-combined.jsonl','r') as f:\n",
    "    for row in f:\n",
    "        val.update(row)        \n",
    "len(train.keys()), len(val.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(630237, 69461)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train_qa_triplets = []\n",
    "img_val_qa_triplets = []\n",
    "\n",
    "for k, v in train.items():\n",
    "    triplets = []\n",
    "    for pair in v['qa-pairs']:\n",
    "        triplets.append([k, pair[0], pair[1]])\n",
    "    for pair in v['qa-binary']:\n",
    "        triplets.append([k, pair[0], pair[1]])\n",
    "    img_train_qa_triplets.extend(triplets)\n",
    "    \n",
    "for k, v in val.items():\n",
    "    triplets = []\n",
    "    for pair in v['qa-pairs']:\n",
    "        triplets.append([k, pair[0], pair[1]])\n",
    "    for pair in v['qa-binary']:\n",
    "        triplets.append([k, pair[0], pair[1]])\n",
    "    img_val_qa_triplets.extend(triplets)\n",
    "\n",
    "\n",
    "len(img_train_qa_triplets), len(img_val_qa_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(630237, 3) (69461, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.DataFrame(img_train_qa_triplets, columns=['img_id','question','answer'])\n",
    "val_df = pd.DataFrame(img_val_qa_triplets, columns=['img_id','question','answer'])\n",
    "\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1025"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_train_labels = set(train_df.answer.to_list())\n",
    "vqa_val_labels = set(val_df.answer.to_list())\n",
    "\n",
    "intersecting_labels = vqa_train_labels.intersection(vqa_val_labels)\n",
    "intersecting_labels = set([label for label in intersecting_labels if len(label)>0])\n",
    "\n",
    "len(intersecting_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((622557, 3), (69079, 3))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[train_df.answer.isin(intersecting_labels)]\n",
    "val_df = val_df[val_df.answer.isin(intersecting_labels)]\n",
    "\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "c = collections.Counter(train_df.answer)\n",
    "c = collections.Counter({k: c for k, c in c.items() if c >= 5})\n",
    "\n",
    "valid_labels = c.keys()\n",
    "'yes' in valid_labels, 'dadas' in valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((621960, 3), (68761, 3))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[train_df.answer.isin(valid_labels)]\n",
    "val_df = val_df[val_df.answer.isin(valid_labels)]\n",
    "\n",
    "train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/ADE20K-QA/vqa_train_df.tsv\", sep=\"\\t\",index=False)\n",
    "val_df.to_csv(\"../data/ADE20K-QA/vqa_val_df.tsv\", sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>What part of the room is a photo frame attache...</td>\n",
       "      <td>wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>Along with a window, what is on the inside of ...</td>\n",
       "      <td>curtain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>Along with blankets, what is on the bed?</td>\n",
       "      <td>pillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>In this picture I can see the inside view of w...</td>\n",
       "      <td>room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>What do I see through the window?</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630232</th>\n",
       "      <td>ADE_train_00016390</td>\n",
       "      <td>In this image, can you see a drawer?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630233</th>\n",
       "      <td>ADE_train_00016390</td>\n",
       "      <td>Can you see candies?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630234</th>\n",
       "      <td>ADE_train_00016390</td>\n",
       "      <td>Does this picture have a telephone?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630235</th>\n",
       "      <td>ADE_train_00016390</td>\n",
       "      <td>Is there a pulpit?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630236</th>\n",
       "      <td>ADE_train_00016390</td>\n",
       "      <td>Do you see a synthesizer in this picture?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>621960 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    img_id                                           question  \\\n",
       "0       ADE_train_00003661  What part of the room is a photo frame attache...   \n",
       "1       ADE_train_00003661  Along with a window, what is on the inside of ...   \n",
       "2       ADE_train_00003661           Along with blankets, what is on the bed?   \n",
       "3       ADE_train_00003661  In this picture I can see the inside view of w...   \n",
       "4       ADE_train_00003661                  What do I see through the window?   \n",
       "...                    ...                                                ...   \n",
       "630232  ADE_train_00016390               In this image, can you see a drawer?   \n",
       "630233  ADE_train_00016390                               Can you see candies?   \n",
       "630234  ADE_train_00016390                Does this picture have a telephone?   \n",
       "630235  ADE_train_00016390                                 Is there a pulpit?   \n",
       "630236  ADE_train_00016390          Do you see a synthesizer in this picture?   \n",
       "\n",
       "         answer  \n",
       "0          wall  \n",
       "1       curtain  \n",
       "2        pillow  \n",
       "3          room  \n",
       "4           sky  \n",
       "...         ...  \n",
       "630232       no  \n",
       "630233       no  \n",
       "630234       no  \n",
       "630235       no  \n",
       "630236       no  \n",
       "\n",
       "[621960 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train_caption_pairs = []\n",
    "img_val_caption_pairs = []\n",
    "\n",
    "for k, v in train.items():\n",
    "    caption = v['caption']\n",
    "    img_train_caption_pairs.append((k, caption))\n",
    "    \n",
    "\n",
    "for k, v in val.items():\n",
    "    caption = v['caption']\n",
    "    img_val_caption_pairs.append((k, caption))\n",
    "\n",
    "len(img_train_caption_pairs), len(img_val_caption_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20210, 2), (2000, 2))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_train = pd.DataFrame(img_train_caption_pairs, columns=['img_id','caption'])\n",
    "caption_val = pd.DataFrame(img_val_caption_pairs, columns=['img_id','caption'])\n",
    "\n",
    "caption_train.shape, caption_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_train.to_csv(\"../data/ADE20K-QA/caption_train_df.tsv\", sep=\"\\t\",index=False)\n",
    "caption_val.to_csv(\"../data/ADE20K-QA/caption_val_df.tsv\", sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADE_val_00001410</td>\n",
       "      <td>In this picture I can see the vehicles on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADE_val_00001113</td>\n",
       "      <td>In this image I can see water and I can also s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADE_val_00000424</td>\n",
       "      <td>In this image we can see a table, light, book,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADE_val_00001210</td>\n",
       "      <td>This image is taken outdoors. At the top of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADE_val_00001480</td>\n",
       "      <td>In this image, there are cupboards, stove, mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>ADE_val_00000922</td>\n",
       "      <td>In the image there is a wall and there are two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>ADE_val_00001449</td>\n",
       "      <td>In the foreground, I can see a fire hydrant, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>ADE_val_00001098</td>\n",
       "      <td>In this picture I can see a toilet on the floo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>ADE_val_00001735</td>\n",
       "      <td>In the image I can see the the view of a place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>ADE_val_00000891</td>\n",
       "      <td>In this image we can see a clock on a tower, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                img_id                                            caption\n",
       "0     ADE_val_00001410  In this picture I can see the vehicles on the ...\n",
       "1     ADE_val_00001113  In this image I can see water and I can also s...\n",
       "2     ADE_val_00000424  In this image we can see a table, light, book,...\n",
       "3     ADE_val_00001210  This image is taken outdoors. At the top of th...\n",
       "4     ADE_val_00001480  In this image, there are cupboards, stove, mic...\n",
       "...                ...                                                ...\n",
       "1995  ADE_val_00000922  In the image there is a wall and there are two...\n",
       "1996  ADE_val_00001449  In the foreground, I can see a fire hydrant, a...\n",
       "1997  ADE_val_00001098  In this picture I can see a toilet on the floo...\n",
       "1998  ADE_val_00001735  In the image I can see the the view of a place...\n",
       "1999  ADE_val_00000891  In this image we can see a clock on a tower, c...\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../data/ADE20K-QA/caption_val_df.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
