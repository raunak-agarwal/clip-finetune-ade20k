{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "# !python3 -m spacy download en_core_web_sm\n",
    "from pprint import pprint\n",
    "import string\n",
    "import numpy as np\n",
    "import jsonlines\n",
    "p=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "BASEPATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = dict()\n",
    "val = dict()\n",
    "\n",
    "basepath = \"../data/\"\n",
    "\n",
    "with jsonlines.open(basepath+'ade20k-train-combined.jsonl','r') as f:\n",
    "    for row in f:\n",
    "        train.update(row)        \n",
    "\n",
    "with jsonlines.open(basepath+'ade20k-val-combined.jsonl','r') as f:\n",
    "    for row in f:\n",
    "        val.update(row)        \n",
    "len(train.keys()), len(val.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train).T.reset_index(level=0)[['index','caption']].rename(columns={'index':'img_id'})\n",
    "val = pd.DataFrame(val).T.reset_index(level=0)[['index','caption']].rename(columns={'index':'img_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>In this picture I can see the inside view of a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADE_train_00003722</td>\n",
       "      <td>This is a picture taken inside of a room in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADE_train_00013827</td>\n",
       "      <td>In this image we can see monitors, keyboards, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADE_train_00009835</td>\n",
       "      <td>There is a building at the bottom of this imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADE_train_00002932</td>\n",
       "      <td>At the bottom of the image there is a bed with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20205</th>\n",
       "      <td>ADE_train_00014320</td>\n",
       "      <td>In this picture I can observe some dried leave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20206</th>\n",
       "      <td>ADE_train_00006757</td>\n",
       "      <td>In this picture we can see chairs, objects on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20207</th>\n",
       "      <td>ADE_train_00002858</td>\n",
       "      <td>In this image. In the foreground, I can see a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20208</th>\n",
       "      <td>ADE_train_00003918</td>\n",
       "      <td>In this image, there are different color pillo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20209</th>\n",
       "      <td>ADE_train_00016390</td>\n",
       "      <td>In this picture we can see there are buildings...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20210 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   img_id                                            caption\n",
       "0      ADE_train_00003661  In this picture I can see the inside view of a...\n",
       "1      ADE_train_00003722  This is a picture taken inside of a room in th...\n",
       "2      ADE_train_00013827  In this image we can see monitors, keyboards, ...\n",
       "3      ADE_train_00009835  There is a building at the bottom of this imag...\n",
       "4      ADE_train_00002932  At the bottom of the image there is a bed with...\n",
       "...                   ...                                                ...\n",
       "20205  ADE_train_00014320  In this picture I can observe some dried leave...\n",
       "20206  ADE_train_00006757  In this picture we can see chairs, objects on ...\n",
       "20207  ADE_train_00002858  In this image. In the foreground, I can see a ...\n",
       "20208  ADE_train_00003918  In this image, there are different color pillo...\n",
       "20209  ADE_train_00016390  In this picture we can see there are buildings...\n",
       "\n",
       "[20210 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import collections\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "599"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = list(nlp.Defaults.stop_words)\n",
    "# len(sw)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "sw.extend(stopwords.words(\"english\"))\n",
    "\n",
    "bad_words = ['', '0', '1', '10', '2', '3', '4', '5', '6', '7', '8', '9', \n",
    "             'I', 'a', 'above', 'also', 'an', 'and', 'at', 'back', 'background', 'be', 'below',\n",
    "             'behind', 'beside', 'bottom', 'center', 'can', 'corner', 'different','few', 'five', 'foreground','f',\n",
    "             'four', 'front', 'group', 'here', 'image', 'is', 'it', 'in','into','item','items','number',\n",
    "             'just','kind','left', 'like', 'looks', 'look','many', 'middle', \n",
    "             'object', 'objects', 'of', 'one', 'or', 'other', 'on','onto',\n",
    "             'photo', 'picture', 'placed', 'photograph', 'right', \n",
    "             's', 'see', 'seems', 'seen', 'seven', \n",
    "             'side', 'six', 'so', 'some', 't', 'taken', 'that', 'the', 'there',\n",
    "             'them', 'this', 'three', 'they','u',\n",
    "             'to', 'top', 'two', 'view', 'we', 'which', 'under','zero']\n",
    "\n",
    "sw.extend(bad_words)\n",
    "\n",
    "# print(sw)\n",
    "len(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 983863)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "lines = list(train.caption)\n",
    "\n",
    "tokens = []\n",
    "tokens.extend([w.lower() for line in lines for w in wordpunct_tokenize(line)])\n",
    "\n",
    "len(lines), len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3018"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unigrams\n",
    "\n",
    "unigrams = collections.Counter(tokens).most_common(5000)\n",
    "unigrams = [t[0] for t in unigrams]\n",
    "unigrams = [row.translate(str.maketrans('', '', p)) for row in unigrams]\n",
    "unigrams = [row for row in unigrams if len(row)>1]\n",
    "unigrams = list(set(unigrams))\n",
    "unigrams = [row for row in unigrams if row not in sw]\n",
    "# data.extend(unigrams)\n",
    "len(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7027"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bigrams\n",
    "\n",
    "# bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "# finder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n",
    "# finder.apply_freq_filter(5)\n",
    "# bigram_tuples = finder.nbest(bigram_measures.pmi, 10000)\n",
    "# bigrams = []\n",
    "# for bigram_tuple in bigram_tuples:\n",
    "#     if not(bigram_tuple[0] in sw and bigram_tuple[1] in sw):\n",
    "#         bigrams.append(' '.join(bigram_tuple))\n",
    "# bigrams = [row.translate(str.maketrans('', '', p)).strip() for row in bigrams]\n",
    "# trigrams = [row for row in bigrams if len(row)>0]\n",
    "\n",
    "# # data.extend(bigrams)\n",
    "# len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "# finder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n",
    "# finder.apply_freq_filter(5)\n",
    "# trigram_tuples = finder.nbest(trigram_measures.pmi, 10000)\n",
    "# trigrams = []\n",
    "# for trigram_tuple in trigram_tuples:\n",
    "#     c = 0\n",
    "#     for t in trigram_tuple:\n",
    "#         if t in sw:\n",
    "#             c+=1\n",
    "#     if c < 2:\n",
    "#         trigrams.append(' '.join(trigram_tuple))\n",
    "# trigrams = [row.translate(str.maketrans('', '', p)).strip() for row in trigrams]\n",
    "# trigrams = [row for row in trigrams if len(row)>0]\n",
    "\n",
    "# len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "coco_file = requests.get(\"https://the-eye.eu/public/AI/models/antarctic-captions/postcache.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = []\n",
    "# for line in coco_file.text.splitlines():\n",
    "#     c.append(line)\n",
    "    \n",
    "# len(c)\n",
    "\n",
    "# coco = []\n",
    "# for row in c:\n",
    "#     l = row.split()\n",
    "#     if len(l) == 1:\n",
    "#         if row not in sw: \n",
    "#             coco.append(row)\n",
    "# #     if len(l) == 2:\n",
    "# #         if not(l[0] in sw and l[1] in sw):\n",
    "# #             coco.append(row)\n",
    "            \n",
    "# coco = [row.translate(str.maketrans('', '', p)).strip() for row in coco]\n",
    "# coco = [row for row in coco if len(row)>0]\n",
    "\n",
    "# len(coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3018"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "data.extend(unigrams)\n",
    "# data.extend(bigrams)\n",
    "# data.extend(trigrams)\n",
    "# data.extend(coco)\n",
    "grams = list(set(data))\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dataset-exploration/clip-finetune-ade20k/antarctic-captions\n"
     ]
    }
   ],
   "source": [
    "%cd ../antarctic-captions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from CLIP import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "CHECKPOINT = \"../training-scripts/open_clip/logs/low-lr-low-wd-rn50/checkpoints/epoch_6.pt\"\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\",device=device,jit=True) #Must set jit=False for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(CHECKPOINT)\n",
    "\n",
    "# # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "checkpoint['state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "checkpoint['state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "checkpoint['state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(texts):\n",
    "    all_text_features = []\n",
    "    for i, x in enumerate(batch(texts, 3000)):\n",
    "        print(\"Batch: \", i)\n",
    "        with torch.no_grad():\n",
    "            text_inputs = torch.cat([clip.tokenize(text) for text in x]).to(device)\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "            all_text_features.extend(text_features.cpu().numpy())\n",
    "\n",
    "    return all_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch:  1\n"
     ]
    }
   ],
   "source": [
    "embeddings = get_text_features(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3018, 3018)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings), len(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3018, 1024)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = \"/home/users/ragarwal/project/dataset-exploration/clip-finetune-ade20k/outputs/ade20k-clean-index/\"\n",
    "np.save(d+\"emb.npy\", np.array(embeddings))\n",
    "\n",
    "with open(d+\"grams.txt\", 'w') as f:\n",
    "    for row in grams:\n",
    "        f.write(row+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(zip(grams, embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def test_text(t, dd):\n",
    "    emb = get_text_features([t])[0]\n",
    "    k = list(dd.keys())\n",
    "    v = list(dd.values())\n",
    "    predict = semantic_search(torch.Tensor(emb), torch.Tensor(v), top_k=50)[0]\n",
    "    \n",
    "    for i in predict:\n",
    "        print(k[i['corpus_id']], i['score'])\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "guy 0.8982160687446594\n",
      "badly 0.8881394267082214\n",
      "bad 0.8864423632621765\n",
      "aboy 0.8847781419754028\n",
      "hes 0.8762815594673157\n",
      "daddy 0.8754550218582153\n",
      "babay 0.8750624060630798\n",
      "baby 0.8746769428253174\n",
      "killer 0.8727972507476807\n",
      "gunman 0.8722986578941345\n",
      "boul 0.8717570900917053\n",
      "goody 0.8699916005134583\n",
      "kill 0.8697551488876343\n",
      "cool 0.8686541318893433\n",
      "boss 0.8684195876121521\n",
      "highly 0.8681061863899231\n",
      "goodly 0.8675404191017151\n",
      "cooly 0.8669133186340332\n",
      "heard 0.8658984303474426\n",
      "baord 0.8658579587936401\n",
      "good 0.8655068874359131\n",
      "yeah 0.8654959797859192\n",
      "mister 0.865454912185669\n",
      "thats 0.8652703166007996\n",
      "killed 0.8650326728820801\n",
      "told 0.8645067811012268\n",
      "babe 0.864257276058197\n",
      "king 0.8641729950904846\n",
      "girll 0.8634682297706604\n",
      "mans 0.8628709316253662\n",
      "hard 0.8627486228942871\n",
      "sayd 0.8623270392417908\n",
      "said 0.8620197772979736\n",
      "beast 0.8617669343948364\n",
      "style 0.8617085814476013\n",
      "boddy 0.8610024452209473\n",
      "reminds 0.8609949350357056\n",
      "personified 0.8606639504432678\n",
      "slyly 0.860624372959137\n",
      "swag 0.8604247570037842\n",
      "word 0.8603850603103638\n",
      "dammed 0.8603394031524658\n",
      "sounds 0.8603218793869019\n",
      "standard 0.8602695465087891\n",
      "albeit 0.8600030541419983\n",
      "figured 0.8597576022148132\n",
      "evil 0.8596580624580383\n",
      "likek 0.8593953251838684\n",
      "boyfriend 0.8593382239341736\n",
      "quality 0.8589629530906677\n"
     ]
    }
   ],
   "source": [
    "x = test_text(\"bad guy \", d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpus_id': 1608, 'score': 0.8984373211860657}"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.10596 ,  0.3735  ,  0.42    , ..., -0.02908 ,  0.4182  ,\n",
       "        -0.0876  ],\n",
       "       [-0.09143 ,  0.2312  , -0.215   , ...,  0.429   , -0.1384  ,\n",
       "         0.04523 ],\n",
       "       [ 0.0877  ,  0.1052  ,  0.05008 , ..., -0.1053  , -0.3806  ,\n",
       "        -0.179   ],\n",
       "       ...,\n",
       "       [ 0.07495 ,  0.2932  , -0.1362  , ...,  0.11414 ,  0.0569  ,\n",
       "        -0.4634  ],\n",
       "       [-0.3123  , -0.006054, -0.2031  , ..., -0.3076  , -0.1565  ,\n",
       "        -0.1428  ],\n",
       "       [ 0.2266  ,  0.178   , -0.1002  , ..., -0.0394  ,  0.2021  ,\n",
       "         0.00836 ]], dtype=float16)"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(d.values())).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "def test_img(dd, img_features):\n",
    "    \n",
    "    k = list(dd.keys())\n",
    "    v = list(dd.values())\n",
    "    predict = semantic_search(torch.Tensor(img_features), torch.Tensor(v), top_k=20)[0]\n",
    "    \n",
    "    for i in predict:\n",
    "        print(k[i['corpus_id']], i['score'])\n",
    "\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_embedding_pairs = np.load(\"../outputs/clip-embeddings/finetuned-img-embeddings-train.npy\", allow_pickle=True)\n",
    "train_img_embedding_pairs = train_img_embedding_pairs.item()\n",
    "\n",
    "val_img_embedding_pairs = np.load(\"../outputs/clip-embeddings/finetuned-img-embeddings-val.npy\", allow_pickle=True)\n",
    "val_img_embedding_pairs = val_img_embedding_pairs.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_img_embedding_pairs['ADE_train_00008485']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotlights 0.22547604143619537\n",
      "leds 0.21454814076423645\n",
      "garage 0.2127702385187149\n",
      "hid 0.2120674103498459\n",
      "headlights 0.2069370150566101\n",
      "headlamps 0.2043430209159851\n",
      "spotlighting 0.20350639522075653\n",
      "garages 0.2023019641637802\n",
      "downlight 0.19860047101974487\n",
      "sidelights 0.1964808702468872\n",
      "installed 0.19610333442687988\n",
      "indicators 0.19545799493789673\n",
      "lights 0.19388195872306824\n",
      "led 0.1924658715724945\n",
      "extinguishers 0.1918090283870697\n",
      "crusiers 0.19167503714561462\n",
      "headlight 0.1911206990480423\n",
      "ligths 0.18996240198612213\n",
      "lightings 0.18899071216583252\n",
      "mazda 0.1873607337474823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'corpus_id': 1343, 'score': 0.22547604143619537},\n",
       " {'corpus_id': 11121, 'score': 0.21454814076423645},\n",
       " {'corpus_id': 11160, 'score': 0.2127702385187149},\n",
       " {'corpus_id': 19717, 'score': 0.2120674103498459},\n",
       " {'corpus_id': 22148, 'score': 0.2069370150566101},\n",
       " {'corpus_id': 22126, 'score': 0.2043430209159851},\n",
       " {'corpus_id': 6834, 'score': 0.20350639522075653},\n",
       " {'corpus_id': 8232, 'score': 0.2023019641637802},\n",
       " {'corpus_id': 4487, 'score': 0.19860047101974487},\n",
       " {'corpus_id': 18602, 'score': 0.1964808702468872},\n",
       " {'corpus_id': 4674, 'score': 0.19610333442687988},\n",
       " {'corpus_id': 3515, 'score': 0.19545799493789673},\n",
       " {'corpus_id': 3155, 'score': 0.19388195872306824},\n",
       " {'corpus_id': 17199, 'score': 0.1924658715724945},\n",
       " {'corpus_id': 3340, 'score': 0.1918090283870697},\n",
       " {'corpus_id': 7664, 'score': 0.19167503714561462},\n",
       " {'corpus_id': 9169, 'score': 0.1911206990480423},\n",
       " {'corpus_id': 2011, 'score': 0.18996240198612213},\n",
       " {'corpus_id': 11654, 'score': 0.18899071216583252},\n",
       " {'corpus_id': 9799, 'score': 0.1873607337474823}]"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img(d, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03397,  0.04382, -0.03583, ...,  0.0169 ,  0.0589 ,  0.01245],\n",
       "      dtype=float16)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
