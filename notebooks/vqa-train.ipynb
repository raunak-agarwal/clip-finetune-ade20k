{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightning-bolts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEPATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(BASEPATH+\"ADE20K-QA/vqa_train_df.tsv\", sep=\"\\t\")\n",
    "val_df = pd.read_csv(BASEPATH+\"ADE20K-QA/vqa_val_df.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_id</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>What part of the room is a photo frame attache...</td>\n",
       "      <td>wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>Along with a window, what is on the inside of ...</td>\n",
       "      <td>curtain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>Along with blankets, what is on the bed?</td>\n",
       "      <td>pillow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>In this picture I can see the inside view of w...</td>\n",
       "      <td>room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADE_train_00003661</td>\n",
       "      <td>What do I see through the window?</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               img_id                                           question  \\\n",
       "0  ADE_train_00003661  What part of the room is a photo frame attache...   \n",
       "1  ADE_train_00003661  Along with a window, what is on the inside of ...   \n",
       "2  ADE_train_00003661           Along with blankets, what is on the bed?   \n",
       "3  ADE_train_00003661  In this picture I can see the inside view of w...   \n",
       "4  ADE_train_00003661                  What do I see through the window?   \n",
       "\n",
       "    answer  \n",
       "0     wall  \n",
       "1  curtain  \n",
       "2   pillow  \n",
       "3     room  \n",
       "4      sky  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20210, 2000)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_img_paths = glob.glob(BASEPATH+\"ADE20K-pairs/final-pairs/unsplit/train/*.jpg\")\n",
    "val_img_paths = glob.glob(BASEPATH+\"ADE20K-pairs/final-pairs/unsplit/val/*.jpg\")\n",
    "\n",
    "train_img_ids = [path.split(\".jpg\")[0].split(\"/\")[-1] for path in train_img_paths]\n",
    "val_img_ids = [path.split(\".jpg\")[0].split(\"/\")[-1] for path in val_img_paths]\n",
    "\n",
    "\n",
    "len(train_img_ids), len(val_img_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = pd.DataFrame([(i,j) for i, j in zip(train_img_ids, train_img_paths)], columns=['img_id','img_path'])\n",
    "y = pd.DataFrame([(i,j) for i, j in zip(val_img_ids, val_img_paths)], columns=['img_id','img_path'])\n",
    "\n",
    "s = set(train_df.img_id)\n",
    "t = set(x.img_id)\n",
    "u = set(val_df.img_id)\n",
    "v = set(y.img_id)\n",
    "\n",
    "train_id_to_path = dict(zip(train_img_ids, train_img_paths))\n",
    "val_id_to_path = dict(zip(val_img_ids, val_img_paths))\n",
    "s==t, u==v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/dataset-exploration/clip-finetune-ade20k/antarctic-captions\n"
     ]
    }
   ],
   "source": [
    "%cd ../antarctic-captions\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from CLIP import clip\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CHECKPOINT = \"../training-scripts/open_clip/logs/low-lr-low-wd-rn50/checkpoints/epoch_6.pt\"\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"RN50\",device=device,jit=True) #Must set jit=False for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(CHECKPOINT)\n",
    "\n",
    "# # # Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "# checkpoint['state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "# checkpoint['state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "# checkpoint['state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "# model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(filepath):\n",
    "    return preprocess(Image.open(filepath)).unsqueeze(0).to(device)\n",
    "\n",
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def generate_splits(names):\n",
    "    splits = []\n",
    "    split_threshold = int(len(names)/6)\n",
    "    print(\"size of each split: \", split_threshold)\n",
    "    for i in range(6):\n",
    "        first = i*split_threshold\n",
    "        last = (i+1)*split_threshold\n",
    "        if i==5:\n",
    "            last=len(names)\n",
    "        splits.append(names[first:last])\n",
    "\n",
    "    return splits\n",
    "\n",
    "def read_split(split):\n",
    "    img_ids, names = split.img_id, split.img_path\n",
    "    images = []\n",
    "    img_emb = []\n",
    "    for i, filepath in enumerate(names):\n",
    "        if not i % 200:\n",
    "            print(i)\n",
    "        img = read_image(filepath)\n",
    "        images.append(img)\n",
    "    with torch.no_grad():\n",
    "        for x in batch(images, 256):\n",
    "            img_emb.append(model.encode_image(torch.cat(x)))\n",
    "    #img_ids = torch.cat(tuple(img_ids)).cpu().numpy()\n",
    "    img_emb = torch.cat(img_emb).cpu().numpy()\n",
    "    return img_ids, names, img_emb\n",
    "\n",
    "def read_all(splits):\n",
    "    img_ids = []\n",
    "    image_embeddings = []\n",
    "    filenames = []\n",
    "    for i, s in enumerate(splits):\n",
    "        print(\"Now encoding split #\", i)\n",
    "        img_ids_, names, embeddings = read_split(s)\n",
    "        img_ids.extend(img_ids_)\n",
    "        filenames.extend(names)\n",
    "        image_embeddings.extend(embeddings)\n",
    "        \n",
    "    return img_ids, filenames, image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of each split:  3368\n",
      "Now encoding split # 0\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "Now encoding split # 1\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "Now encoding split # 2\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "Now encoding split # 3\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "Now encoding split # 4\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "Now encoding split # 5\n",
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "size of each split:  333\n",
      "Now encoding split # 0\n",
      "0\n",
      "200\n",
      "Now encoding split # 1\n",
      "0\n",
      "200\n",
      "Now encoding split # 2\n",
      "0\n",
      "200\n",
      "Now encoding split # 3\n",
      "0\n",
      "200\n",
      "Now encoding split # 4\n",
      "0\n",
      "200\n",
      "Now encoding split # 5\n",
      "0\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "train_ids, _, train_img_embeddings = read_all(generate_splits(x))\n",
    "val_ids, _, val_img_embeddings = read_all(generate_splits(y))\n",
    "# images = [read_image(filepath) for filepath in img_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_embedding_pairs = dict()\n",
    "for i, img_id in enumerate(train_ids): \n",
    "    train_img_embedding_pairs[img_id] = train_img_embeddings[i]\n",
    "\n",
    "val_img_embedding_pairs = dict()\n",
    "for i, img_id in enumerate(val_ids): \n",
    "    val_img_embedding_pairs[img_id] = val_img_embeddings[i]\n",
    "\n",
    "np.save(\"../outputs/clip-embeddings/pretrained-img-embeddings-train.npy\", train_img_embedding_pairs)\n",
    "np.save(\"../outputs/clip-embeddings/pretrained-img-embeddings-val.npy\", val_img_embedding_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_img_embedding_pairs = np.load(\"../outputs/clip-embeddings/finetuned-img-embeddings-train.npy\", allow_pickle=True)\n",
    "# train_img_embedding_pairs = train_img_embedding_pairs.item()\n",
    "\n",
    "# val_img_embedding_pairs = np.load(\"../outputs/clip-embeddings/finetuned-img-embeddings-val.npy\", allow_pickle=True)\n",
    "# val_img_embedding_pairs = val_img_embedding_pairs.item()\n",
    "\n",
    "train_img_embedding_pairs = np.load(\"../outputs/clip-embeddings/pretrained-img-embeddings-train.npy\", allow_pickle=True)\n",
    "train_img_embedding_pairs = train_img_embedding_pairs.item()\n",
    "\n",
    "val_img_embedding_pairs = np.load(\"../outputs/clip-embeddings/pretrained-img-embeddings-val.npy\", allow_pickle=True)\n",
    "val_img_embedding_pairs = val_img_embedding_pairs.item()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_features(texts):\n",
    "    all_text_features = []\n",
    "    for i, x in enumerate(batch(texts, 3000)):\n",
    "        print(\"Batch: \", i)\n",
    "        with torch.no_grad():\n",
    "            text_inputs = torch.cat([clip.tokenize(text) for text in x]).to(device)\n",
    "            text_features = model.encode_text(text_inputs)\n",
    "            all_text_features.extend(text_features.cpu().numpy())\n",
    "\n",
    "    return all_text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_features(train_df.question[0:5])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_text_features([train_df.question[0]])\n",
    "b = train_img_embedding_pairs[train_df.img_id[0]]\n",
    "np.concatenate((a, b), axis=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(df, train=True):\n",
    "    text_features_all = get_text_features(df.question)\n",
    "    print(len(text_features_all))\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, row in enumerate(df.itertuples()):\n",
    "        if not i % 500: print(\"i = \", i)\n",
    "        img_id = row.img_id\n",
    "        if train:\n",
    "            img_features = train_img_embedding_pairs[img_id]\n",
    "        else:\n",
    "            img_features = val_img_embedding_pairs[img_id]\n",
    "        text_features = text_features_all[i]\n",
    "        features = np.concatenate((text_features, img_features), axis=None)\n",
    "        label = row.answer\n",
    "        \n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "\n",
    "    return X, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "Batch:  23\n",
      "Batch:  24\n",
      "Batch:  25\n",
      "Batch:  26\n",
      "Batch:  27\n",
      "Batch:  28\n",
      "Batch:  29\n",
      "Batch:  30\n",
      "Batch:  31\n",
      "Batch:  32\n",
      "Batch:  33\n",
      "Batch:  34\n",
      "Batch:  35\n",
      "Batch:  36\n",
      "Batch:  37\n",
      "Batch:  38\n",
      "Batch:  39\n",
      "Batch:  40\n",
      "Batch:  41\n",
      "Batch:  42\n",
      "Batch:  43\n",
      "Batch:  44\n",
      "Batch:  45\n",
      "Batch:  46\n",
      "Batch:  47\n",
      "Batch:  48\n",
      "Batch:  49\n",
      "Batch:  50\n",
      "Batch:  51\n",
      "Batch:  52\n",
      "Batch:  53\n",
      "Batch:  54\n",
      "Batch:  55\n",
      "Batch:  56\n",
      "Batch:  57\n",
      "Batch:  58\n",
      "Batch:  59\n",
      "Batch:  60\n",
      "Batch:  61\n",
      "Batch:  62\n",
      "Batch:  63\n",
      "Batch:  64\n",
      "Batch:  65\n",
      "Batch:  66\n",
      "Batch:  67\n",
      "Batch:  68\n",
      "Batch:  69\n",
      "Batch:  70\n",
      "Batch:  71\n",
      "Batch:  72\n",
      "Batch:  73\n",
      "Batch:  74\n",
      "Batch:  75\n",
      "Batch:  76\n",
      "Batch:  77\n",
      "Batch:  78\n",
      "Batch:  79\n",
      "Batch:  80\n",
      "Batch:  81\n",
      "Batch:  82\n",
      "Batch:  83\n",
      "Batch:  84\n",
      "Batch:  85\n",
      "Batch:  86\n",
      "Batch:  87\n",
      "Batch:  88\n",
      "Batch:  89\n",
      "Batch:  90\n",
      "Batch:  91\n",
      "Batch:  92\n",
      "Batch:  93\n",
      "Batch:  94\n",
      "Batch:  95\n",
      "Batch:  96\n",
      "Batch:  97\n",
      "Batch:  98\n",
      "Batch:  99\n",
      "Batch:  100\n",
      "Batch:  101\n",
      "Batch:  102\n",
      "Batch:  103\n",
      "Batch:  104\n",
      "Batch:  105\n",
      "Batch:  106\n",
      "Batch:  107\n",
      "Batch:  108\n",
      "Batch:  109\n",
      "Batch:  110\n",
      "Batch:  111\n",
      "Batch:  112\n",
      "Batch:  113\n",
      "Batch:  114\n",
      "Batch:  115\n",
      "Batch:  116\n",
      "Batch:  117\n",
      "Batch:  118\n",
      "Batch:  119\n",
      "Batch:  120\n",
      "Batch:  121\n",
      "Batch:  122\n",
      "Batch:  123\n",
      "Batch:  124\n",
      "Batch:  125\n",
      "Batch:  126\n",
      "Batch:  127\n",
      "Batch:  128\n",
      "Batch:  129\n",
      "Batch:  130\n",
      "Batch:  131\n",
      "Batch:  132\n",
      "Batch:  133\n",
      "Batch:  134\n",
      "Batch:  135\n",
      "Batch:  136\n",
      "Batch:  137\n",
      "Batch:  138\n",
      "Batch:  139\n",
      "Batch:  140\n",
      "Batch:  141\n",
      "Batch:  142\n",
      "Batch:  143\n",
      "Batch:  144\n",
      "Batch:  145\n",
      "Batch:  146\n",
      "Batch:  147\n",
      "Batch:  148\n",
      "Batch:  149\n",
      "Batch:  150\n",
      "Batch:  151\n",
      "Batch:  152\n",
      "Batch:  153\n",
      "Batch:  154\n",
      "Batch:  155\n",
      "Batch:  156\n",
      "Batch:  157\n",
      "Batch:  158\n",
      "Batch:  159\n",
      "Batch:  160\n",
      "Batch:  161\n",
      "Batch:  162\n",
      "Batch:  163\n",
      "Batch:  164\n",
      "Batch:  165\n",
      "Batch:  166\n",
      "Batch:  167\n",
      "Batch:  168\n",
      "Batch:  169\n",
      "Batch:  170\n",
      "Batch:  171\n",
      "Batch:  172\n",
      "Batch:  173\n",
      "Batch:  174\n",
      "Batch:  175\n",
      "Batch:  176\n",
      "Batch:  177\n",
      "Batch:  178\n",
      "Batch:  179\n",
      "Batch:  180\n",
      "Batch:  181\n",
      "Batch:  182\n",
      "Batch:  183\n",
      "Batch:  184\n",
      "Batch:  185\n",
      "Batch:  186\n",
      "Batch:  187\n",
      "Batch:  188\n",
      "Batch:  189\n",
      "Batch:  190\n",
      "Batch:  191\n",
      "Batch:  192\n",
      "Batch:  193\n",
      "Batch:  194\n",
      "Batch:  195\n",
      "Batch:  196\n",
      "Batch:  197\n",
      "Batch:  198\n",
      "Batch:  199\n",
      "Batch:  200\n",
      "Batch:  201\n",
      "Batch:  202\n",
      "Batch:  203\n",
      "Batch:  204\n",
      "Batch:  205\n",
      "Batch:  206\n",
      "Batch:  207\n",
      "621960\n",
      "i =  0\n",
      "i =  500\n",
      "i =  1000\n",
      "i =  1500\n",
      "i =  2000\n",
      "i =  2500\n",
      "i =  3000\n",
      "i =  3500\n",
      "i =  4000\n",
      "i =  4500\n",
      "i =  5000\n",
      "i =  5500\n",
      "i =  6000\n",
      "i =  6500\n",
      "i =  7000\n",
      "i =  7500\n",
      "i =  8000\n",
      "i =  8500\n",
      "i =  9000\n",
      "i =  9500\n",
      "i =  10000\n",
      "i =  10500\n",
      "i =  11000\n",
      "i =  11500\n",
      "i =  12000\n",
      "i =  12500\n",
      "i =  13000\n",
      "i =  13500\n",
      "i =  14000\n",
      "i =  14500\n",
      "i =  15000\n",
      "i =  15500\n",
      "i =  16000\n",
      "i =  16500\n",
      "i =  17000\n",
      "i =  17500\n",
      "i =  18000\n",
      "i =  18500\n",
      "i =  19000\n",
      "i =  19500\n",
      "i =  20000\n",
      "i =  20500\n",
      "i =  21000\n",
      "i =  21500\n",
      "i =  22000\n",
      "i =  22500\n",
      "i =  23000\n",
      "i =  23500\n",
      "i =  24000\n",
      "i =  24500\n",
      "i =  25000\n",
      "i =  25500\n",
      "i =  26000\n",
      "i =  26500\n",
      "i =  27000\n",
      "i =  27500\n",
      "i =  28000\n",
      "i =  28500\n",
      "i =  29000\n",
      "i =  29500\n",
      "i =  30000\n",
      "i =  30500\n",
      "i =  31000\n",
      "i =  31500\n",
      "i =  32000\n",
      "i =  32500\n",
      "i =  33000\n",
      "i =  33500\n",
      "i =  34000\n",
      "i =  34500\n",
      "i =  35000\n",
      "i =  35500\n",
      "i =  36000\n",
      "i =  36500\n",
      "i =  37000\n",
      "i =  37500\n",
      "i =  38000\n",
      "i =  38500\n",
      "i =  39000\n",
      "i =  39500\n",
      "i =  40000\n",
      "i =  40500\n",
      "i =  41000\n",
      "i =  41500\n",
      "i =  42000\n",
      "i =  42500\n",
      "i =  43000\n",
      "i =  43500\n",
      "i =  44000\n",
      "i =  44500\n",
      "i =  45000\n",
      "i =  45500\n",
      "i =  46000\n",
      "i =  46500\n",
      "i =  47000\n",
      "i =  47500\n",
      "i =  48000\n",
      "i =  48500\n",
      "i =  49000\n",
      "i =  49500\n",
      "i =  50000\n",
      "i =  50500\n",
      "i =  51000\n",
      "i =  51500\n",
      "i =  52000\n",
      "i =  52500\n",
      "i =  53000\n",
      "i =  53500\n",
      "i =  54000\n",
      "i =  54500\n",
      "i =  55000\n",
      "i =  55500\n",
      "i =  56000\n",
      "i =  56500\n",
      "i =  57000\n",
      "i =  57500\n",
      "i =  58000\n",
      "i =  58500\n",
      "i =  59000\n",
      "i =  59500\n",
      "i =  60000\n",
      "i =  60500\n",
      "i =  61000\n",
      "i =  61500\n",
      "i =  62000\n",
      "i =  62500\n",
      "i =  63000\n",
      "i =  63500\n",
      "i =  64000\n",
      "i =  64500\n",
      "i =  65000\n",
      "i =  65500\n",
      "i =  66000\n",
      "i =  66500\n",
      "i =  67000\n",
      "i =  67500\n",
      "i =  68000\n",
      "i =  68500\n",
      "i =  69000\n",
      "i =  69500\n",
      "i =  70000\n",
      "i =  70500\n",
      "i =  71000\n",
      "i =  71500\n",
      "i =  72000\n",
      "i =  72500\n",
      "i =  73000\n",
      "i =  73500\n",
      "i =  74000\n",
      "i =  74500\n",
      "i =  75000\n",
      "i =  75500\n",
      "i =  76000\n",
      "i =  76500\n",
      "i =  77000\n",
      "i =  77500\n",
      "i =  78000\n",
      "i =  78500\n",
      "i =  79000\n",
      "i =  79500\n",
      "i =  80000\n",
      "i =  80500\n",
      "i =  81000\n",
      "i =  81500\n",
      "i =  82000\n",
      "i =  82500\n",
      "i =  83000\n",
      "i =  83500\n",
      "i =  84000\n",
      "i =  84500\n",
      "i =  85000\n",
      "i =  85500\n",
      "i =  86000\n",
      "i =  86500\n",
      "i =  87000\n",
      "i =  87500\n",
      "i =  88000\n",
      "i =  88500\n",
      "i =  89000\n",
      "i =  89500\n",
      "i =  90000\n",
      "i =  90500\n",
      "i =  91000\n",
      "i =  91500\n",
      "i =  92000\n",
      "i =  92500\n",
      "i =  93000\n",
      "i =  93500\n",
      "i =  94000\n",
      "i =  94500\n",
      "i =  95000\n",
      "i =  95500\n",
      "i =  96000\n",
      "i =  96500\n",
      "i =  97000\n",
      "i =  97500\n",
      "i =  98000\n",
      "i =  98500\n",
      "i =  99000\n",
      "i =  99500\n",
      "i =  100000\n",
      "i =  100500\n",
      "i =  101000\n",
      "i =  101500\n",
      "i =  102000\n",
      "i =  102500\n",
      "i =  103000\n",
      "i =  103500\n",
      "i =  104000\n",
      "i =  104500\n",
      "i =  105000\n",
      "i =  105500\n",
      "i =  106000\n",
      "i =  106500\n",
      "i =  107000\n",
      "i =  107500\n",
      "i =  108000\n",
      "i =  108500\n",
      "i =  109000\n",
      "i =  109500\n",
      "i =  110000\n",
      "i =  110500\n",
      "i =  111000\n",
      "i =  111500\n",
      "i =  112000\n",
      "i =  112500\n",
      "i =  113000\n",
      "i =  113500\n",
      "i =  114000\n",
      "i =  114500\n",
      "i =  115000\n",
      "i =  115500\n",
      "i =  116000\n",
      "i =  116500\n",
      "i =  117000\n",
      "i =  117500\n",
      "i =  118000\n",
      "i =  118500\n",
      "i =  119000\n",
      "i =  119500\n",
      "i =  120000\n",
      "i =  120500\n",
      "i =  121000\n",
      "i =  121500\n",
      "i =  122000\n",
      "i =  122500\n",
      "i =  123000\n",
      "i =  123500\n",
      "i =  124000\n",
      "i =  124500\n",
      "i =  125000\n",
      "i =  125500\n",
      "i =  126000\n",
      "i =  126500\n",
      "i =  127000\n",
      "i =  127500\n",
      "i =  128000\n",
      "i =  128500\n",
      "i =  129000\n",
      "i =  129500\n",
      "i =  130000\n",
      "i =  130500\n",
      "i =  131000\n",
      "i =  131500\n",
      "i =  132000\n",
      "i =  132500\n",
      "i =  133000\n",
      "i =  133500\n",
      "i =  134000\n",
      "i =  134500\n",
      "i =  135000\n",
      "i =  135500\n",
      "i =  136000\n",
      "i =  136500\n",
      "i =  137000\n",
      "i =  137500\n",
      "i =  138000\n",
      "i =  138500\n",
      "i =  139000\n",
      "i =  139500\n",
      "i =  140000\n",
      "i =  140500\n",
      "i =  141000\n",
      "i =  141500\n",
      "i =  142000\n",
      "i =  142500\n",
      "i =  143000\n",
      "i =  143500\n",
      "i =  144000\n",
      "i =  144500\n",
      "i =  145000\n",
      "i =  145500\n",
      "i =  146000\n",
      "i =  146500\n",
      "i =  147000\n",
      "i =  147500\n",
      "i =  148000\n",
      "i =  148500\n",
      "i =  149000\n",
      "i =  149500\n",
      "i =  150000\n",
      "i =  150500\n",
      "i =  151000\n",
      "i =  151500\n",
      "i =  152000\n",
      "i =  152500\n",
      "i =  153000\n",
      "i =  153500\n",
      "i =  154000\n",
      "i =  154500\n",
      "i =  155000\n",
      "i =  155500\n",
      "i =  156000\n",
      "i =  156500\n",
      "i =  157000\n",
      "i =  157500\n",
      "i =  158000\n",
      "i =  158500\n",
      "i =  159000\n",
      "i =  159500\n",
      "i =  160000\n",
      "i =  160500\n",
      "i =  161000\n",
      "i =  161500\n",
      "i =  162000\n",
      "i =  162500\n",
      "i =  163000\n",
      "i =  163500\n",
      "i =  164000\n",
      "i =  164500\n",
      "i =  165000\n",
      "i =  165500\n",
      "i =  166000\n",
      "i =  166500\n",
      "i =  167000\n",
      "i =  167500\n",
      "i =  168000\n",
      "i =  168500\n",
      "i =  169000\n",
      "i =  169500\n",
      "i =  170000\n",
      "i =  170500\n",
      "i =  171000\n",
      "i =  171500\n",
      "i =  172000\n",
      "i =  172500\n",
      "i =  173000\n",
      "i =  173500\n",
      "i =  174000\n",
      "i =  174500\n",
      "i =  175000\n",
      "i =  175500\n",
      "i =  176000\n",
      "i =  176500\n",
      "i =  177000\n",
      "i =  177500\n",
      "i =  178000\n",
      "i =  178500\n",
      "i =  179000\n",
      "i =  179500\n",
      "i =  180000\n",
      "i =  180500\n",
      "i =  181000\n",
      "i =  181500\n",
      "i =  182000\n",
      "i =  182500\n",
      "i =  183000\n",
      "i =  183500\n",
      "i =  184000\n",
      "i =  184500\n",
      "i =  185000\n",
      "i =  185500\n",
      "i =  186000\n",
      "i =  186500\n",
      "i =  187000\n",
      "i =  187500\n",
      "i =  188000\n",
      "i =  188500\n",
      "i =  189000\n",
      "i =  189500\n",
      "i =  190000\n",
      "i =  190500\n",
      "i =  191000\n",
      "i =  191500\n",
      "i =  192000\n",
      "i =  192500\n",
      "i =  193000\n",
      "i =  193500\n",
      "i =  194000\n",
      "i =  194500\n",
      "i =  195000\n",
      "i =  195500\n",
      "i =  196000\n",
      "i =  196500\n",
      "i =  197000\n",
      "i =  197500\n",
      "i =  198000\n",
      "i =  198500\n",
      "i =  199000\n",
      "i =  199500\n",
      "i =  200000\n",
      "i =  200500\n",
      "i =  201000\n",
      "i =  201500\n",
      "i =  202000\n",
      "i =  202500\n",
      "i =  203000\n",
      "i =  203500\n",
      "i =  204000\n",
      "i =  204500\n",
      "i =  205000\n",
      "i =  205500\n",
      "i =  206000\n",
      "i =  206500\n",
      "i =  207000\n",
      "i =  207500\n",
      "i =  208000\n",
      "i =  208500\n",
      "i =  209000\n",
      "i =  209500\n",
      "i =  210000\n",
      "i =  210500\n",
      "i =  211000\n",
      "i =  211500\n",
      "i =  212000\n",
      "i =  212500\n",
      "i =  213000\n",
      "i =  213500\n",
      "i =  214000\n",
      "i =  214500\n",
      "i =  215000\n",
      "i =  215500\n",
      "i =  216000\n",
      "i =  216500\n",
      "i =  217000\n",
      "i =  217500\n",
      "i =  218000\n",
      "i =  218500\n",
      "i =  219000\n",
      "i =  219500\n",
      "i =  220000\n",
      "i =  220500\n",
      "i =  221000\n",
      "i =  221500\n",
      "i =  222000\n",
      "i =  222500\n",
      "i =  223000\n",
      "i =  223500\n",
      "i =  224000\n",
      "i =  224500\n",
      "i =  225000\n",
      "i =  225500\n",
      "i =  226000\n",
      "i =  226500\n",
      "i =  227000\n",
      "i =  227500\n",
      "i =  228000\n",
      "i =  228500\n",
      "i =  229000\n",
      "i =  229500\n",
      "i =  230000\n",
      "i =  230500\n",
      "i =  231000\n",
      "i =  231500\n",
      "i =  232000\n",
      "i =  232500\n",
      "i =  233000\n",
      "i =  233500\n",
      "i =  234000\n",
      "i =  234500\n",
      "i =  235000\n",
      "i =  235500\n",
      "i =  236000\n",
      "i =  236500\n",
      "i =  237000\n",
      "i =  237500\n",
      "i =  238000\n",
      "i =  238500\n",
      "i =  239000\n",
      "i =  239500\n",
      "i =  240000\n",
      "i =  240500\n",
      "i =  241000\n",
      "i =  241500\n",
      "i =  242000\n",
      "i =  242500\n",
      "i =  243000\n",
      "i =  243500\n",
      "i =  244000\n",
      "i =  244500\n",
      "i =  245000\n",
      "i =  245500\n",
      "i =  246000\n",
      "i =  246500\n",
      "i =  247000\n",
      "i =  247500\n",
      "i =  248000\n",
      "i =  248500\n",
      "i =  249000\n",
      "i =  249500\n",
      "i =  250000\n",
      "i =  250500\n",
      "i =  251000\n",
      "i =  251500\n",
      "i =  252000\n",
      "i =  252500\n",
      "i =  253000\n",
      "i =  253500\n",
      "i =  254000\n",
      "i =  254500\n",
      "i =  255000\n",
      "i =  255500\n",
      "i =  256000\n",
      "i =  256500\n",
      "i =  257000\n",
      "i =  257500\n",
      "i =  258000\n",
      "i =  258500\n",
      "i =  259000\n",
      "i =  259500\n",
      "i =  260000\n",
      "i =  260500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  261000\n",
      "i =  261500\n",
      "i =  262000\n",
      "i =  262500\n",
      "i =  263000\n",
      "i =  263500\n",
      "i =  264000\n",
      "i =  264500\n",
      "i =  265000\n",
      "i =  265500\n",
      "i =  266000\n",
      "i =  266500\n",
      "i =  267000\n",
      "i =  267500\n",
      "i =  268000\n",
      "i =  268500\n",
      "i =  269000\n",
      "i =  269500\n",
      "i =  270000\n",
      "i =  270500\n",
      "i =  271000\n",
      "i =  271500\n",
      "i =  272000\n",
      "i =  272500\n",
      "i =  273000\n",
      "i =  273500\n",
      "i =  274000\n",
      "i =  274500\n",
      "i =  275000\n",
      "i =  275500\n",
      "i =  276000\n",
      "i =  276500\n",
      "i =  277000\n",
      "i =  277500\n",
      "i =  278000\n",
      "i =  278500\n",
      "i =  279000\n",
      "i =  279500\n",
      "i =  280000\n",
      "i =  280500\n",
      "i =  281000\n",
      "i =  281500\n",
      "i =  282000\n",
      "i =  282500\n",
      "i =  283000\n",
      "i =  283500\n",
      "i =  284000\n",
      "i =  284500\n",
      "i =  285000\n",
      "i =  285500\n",
      "i =  286000\n",
      "i =  286500\n",
      "i =  287000\n",
      "i =  287500\n",
      "i =  288000\n",
      "i =  288500\n",
      "i =  289000\n",
      "i =  289500\n",
      "i =  290000\n",
      "i =  290500\n",
      "i =  291000\n",
      "i =  291500\n",
      "i =  292000\n",
      "i =  292500\n",
      "i =  293000\n",
      "i =  293500\n",
      "i =  294000\n",
      "i =  294500\n",
      "i =  295000\n",
      "i =  295500\n",
      "i =  296000\n",
      "i =  296500\n",
      "i =  297000\n",
      "i =  297500\n",
      "i =  298000\n",
      "i =  298500\n",
      "i =  299000\n",
      "i =  299500\n",
      "i =  300000\n",
      "i =  300500\n",
      "i =  301000\n",
      "i =  301500\n",
      "i =  302000\n",
      "i =  302500\n",
      "i =  303000\n",
      "i =  303500\n",
      "i =  304000\n",
      "i =  304500\n",
      "i =  305000\n",
      "i =  305500\n",
      "i =  306000\n",
      "i =  306500\n",
      "i =  307000\n",
      "i =  307500\n",
      "i =  308000\n",
      "i =  308500\n",
      "i =  309000\n",
      "i =  309500\n",
      "i =  310000\n",
      "i =  310500\n",
      "i =  311000\n",
      "i =  311500\n",
      "i =  312000\n",
      "i =  312500\n",
      "i =  313000\n",
      "i =  313500\n",
      "i =  314000\n",
      "i =  314500\n",
      "i =  315000\n",
      "i =  315500\n",
      "i =  316000\n",
      "i =  316500\n",
      "i =  317000\n",
      "i =  317500\n",
      "i =  318000\n",
      "i =  318500\n",
      "i =  319000\n",
      "i =  319500\n",
      "i =  320000\n",
      "i =  320500\n",
      "i =  321000\n",
      "i =  321500\n",
      "i =  322000\n",
      "i =  322500\n",
      "i =  323000\n",
      "i =  323500\n",
      "i =  324000\n",
      "i =  324500\n",
      "i =  325000\n",
      "i =  325500\n",
      "i =  326000\n",
      "i =  326500\n",
      "i =  327000\n",
      "i =  327500\n",
      "i =  328000\n",
      "i =  328500\n",
      "i =  329000\n",
      "i =  329500\n",
      "i =  330000\n",
      "i =  330500\n",
      "i =  331000\n",
      "i =  331500\n",
      "i =  332000\n",
      "i =  332500\n",
      "i =  333000\n",
      "i =  333500\n",
      "i =  334000\n",
      "i =  334500\n",
      "i =  335000\n",
      "i =  335500\n",
      "i =  336000\n",
      "i =  336500\n",
      "i =  337000\n",
      "i =  337500\n",
      "i =  338000\n",
      "i =  338500\n",
      "i =  339000\n",
      "i =  339500\n",
      "i =  340000\n",
      "i =  340500\n",
      "i =  341000\n",
      "i =  341500\n",
      "i =  342000\n",
      "i =  342500\n",
      "i =  343000\n",
      "i =  343500\n",
      "i =  344000\n",
      "i =  344500\n",
      "i =  345000\n",
      "i =  345500\n",
      "i =  346000\n",
      "i =  346500\n",
      "i =  347000\n",
      "i =  347500\n",
      "i =  348000\n",
      "i =  348500\n",
      "i =  349000\n",
      "i =  349500\n",
      "i =  350000\n",
      "i =  350500\n",
      "i =  351000\n",
      "i =  351500\n",
      "i =  352000\n",
      "i =  352500\n",
      "i =  353000\n",
      "i =  353500\n",
      "i =  354000\n",
      "i =  354500\n",
      "i =  355000\n",
      "i =  355500\n",
      "i =  356000\n",
      "i =  356500\n",
      "i =  357000\n",
      "i =  357500\n",
      "i =  358000\n",
      "i =  358500\n",
      "i =  359000\n",
      "i =  359500\n",
      "i =  360000\n",
      "i =  360500\n",
      "i =  361000\n",
      "i =  361500\n",
      "i =  362000\n",
      "i =  362500\n",
      "i =  363000\n",
      "i =  363500\n",
      "i =  364000\n",
      "i =  364500\n",
      "i =  365000\n",
      "i =  365500\n",
      "i =  366000\n",
      "i =  366500\n",
      "i =  367000\n",
      "i =  367500\n",
      "i =  368000\n",
      "i =  368500\n",
      "i =  369000\n",
      "i =  369500\n",
      "i =  370000\n",
      "i =  370500\n",
      "i =  371000\n",
      "i =  371500\n",
      "i =  372000\n",
      "i =  372500\n",
      "i =  373000\n",
      "i =  373500\n",
      "i =  374000\n",
      "i =  374500\n",
      "i =  375000\n",
      "i =  375500\n",
      "i =  376000\n",
      "i =  376500\n",
      "i =  377000\n",
      "i =  377500\n",
      "i =  378000\n",
      "i =  378500\n",
      "i =  379000\n",
      "i =  379500\n",
      "i =  380000\n",
      "i =  380500\n",
      "i =  381000\n",
      "i =  381500\n",
      "i =  382000\n",
      "i =  382500\n",
      "i =  383000\n",
      "i =  383500\n",
      "i =  384000\n",
      "i =  384500\n",
      "i =  385000\n",
      "i =  385500\n",
      "i =  386000\n",
      "i =  386500\n",
      "i =  387000\n",
      "i =  387500\n",
      "i =  388000\n",
      "i =  388500\n",
      "i =  389000\n",
      "i =  389500\n",
      "i =  390000\n",
      "i =  390500\n",
      "i =  391000\n",
      "i =  391500\n",
      "i =  392000\n",
      "i =  392500\n",
      "i =  393000\n",
      "i =  393500\n",
      "i =  394000\n",
      "i =  394500\n",
      "i =  395000\n",
      "i =  395500\n",
      "i =  396000\n",
      "i =  396500\n",
      "i =  397000\n",
      "i =  397500\n",
      "i =  398000\n",
      "i =  398500\n",
      "i =  399000\n",
      "i =  399500\n",
      "i =  400000\n",
      "i =  400500\n",
      "i =  401000\n",
      "i =  401500\n",
      "i =  402000\n",
      "i =  402500\n",
      "i =  403000\n",
      "i =  403500\n",
      "i =  404000\n",
      "i =  404500\n",
      "i =  405000\n",
      "i =  405500\n",
      "i =  406000\n",
      "i =  406500\n",
      "i =  407000\n",
      "i =  407500\n",
      "i =  408000\n",
      "i =  408500\n",
      "i =  409000\n",
      "i =  409500\n",
      "i =  410000\n",
      "i =  410500\n",
      "i =  411000\n",
      "i =  411500\n",
      "i =  412000\n",
      "i =  412500\n",
      "i =  413000\n",
      "i =  413500\n",
      "i =  414000\n",
      "i =  414500\n",
      "i =  415000\n",
      "i =  415500\n",
      "i =  416000\n",
      "i =  416500\n",
      "i =  417000\n",
      "i =  417500\n",
      "i =  418000\n",
      "i =  418500\n",
      "i =  419000\n",
      "i =  419500\n",
      "i =  420000\n",
      "i =  420500\n",
      "i =  421000\n",
      "i =  421500\n",
      "i =  422000\n",
      "i =  422500\n",
      "i =  423000\n",
      "i =  423500\n",
      "i =  424000\n",
      "i =  424500\n",
      "i =  425000\n",
      "i =  425500\n",
      "i =  426000\n",
      "i =  426500\n",
      "i =  427000\n",
      "i =  427500\n",
      "i =  428000\n",
      "i =  428500\n",
      "i =  429000\n",
      "i =  429500\n",
      "i =  430000\n",
      "i =  430500\n",
      "i =  431000\n",
      "i =  431500\n",
      "i =  432000\n",
      "i =  432500\n",
      "i =  433000\n",
      "i =  433500\n",
      "i =  434000\n",
      "i =  434500\n",
      "i =  435000\n",
      "i =  435500\n",
      "i =  436000\n",
      "i =  436500\n",
      "i =  437000\n",
      "i =  437500\n",
      "i =  438000\n",
      "i =  438500\n",
      "i =  439000\n",
      "i =  439500\n",
      "i =  440000\n",
      "i =  440500\n",
      "i =  441000\n",
      "i =  441500\n",
      "i =  442000\n",
      "i =  442500\n",
      "i =  443000\n",
      "i =  443500\n",
      "i =  444000\n",
      "i =  444500\n",
      "i =  445000\n",
      "i =  445500\n",
      "i =  446000\n",
      "i =  446500\n",
      "i =  447000\n",
      "i =  447500\n",
      "i =  448000\n",
      "i =  448500\n",
      "i =  449000\n",
      "i =  449500\n",
      "i =  450000\n",
      "i =  450500\n",
      "i =  451000\n",
      "i =  451500\n",
      "i =  452000\n",
      "i =  452500\n",
      "i =  453000\n",
      "i =  453500\n",
      "i =  454000\n",
      "i =  454500\n",
      "i =  455000\n",
      "i =  455500\n",
      "i =  456000\n",
      "i =  456500\n",
      "i =  457000\n",
      "i =  457500\n",
      "i =  458000\n",
      "i =  458500\n",
      "i =  459000\n",
      "i =  459500\n",
      "i =  460000\n",
      "i =  460500\n",
      "i =  461000\n",
      "i =  461500\n",
      "i =  462000\n",
      "i =  462500\n",
      "i =  463000\n",
      "i =  463500\n",
      "i =  464000\n",
      "i =  464500\n",
      "i =  465000\n",
      "i =  465500\n",
      "i =  466000\n",
      "i =  466500\n",
      "i =  467000\n",
      "i =  467500\n",
      "i =  468000\n",
      "i =  468500\n",
      "i =  469000\n",
      "i =  469500\n",
      "i =  470000\n",
      "i =  470500\n",
      "i =  471000\n",
      "i =  471500\n",
      "i =  472000\n",
      "i =  472500\n",
      "i =  473000\n",
      "i =  473500\n",
      "i =  474000\n",
      "i =  474500\n",
      "i =  475000\n",
      "i =  475500\n",
      "i =  476000\n",
      "i =  476500\n",
      "i =  477000\n",
      "i =  477500\n",
      "i =  478000\n",
      "i =  478500\n",
      "i =  479000\n",
      "i =  479500\n",
      "i =  480000\n",
      "i =  480500\n",
      "i =  481000\n",
      "i =  481500\n",
      "i =  482000\n",
      "i =  482500\n",
      "i =  483000\n",
      "i =  483500\n",
      "i =  484000\n",
      "i =  484500\n",
      "i =  485000\n",
      "i =  485500\n",
      "i =  486000\n",
      "i =  486500\n",
      "i =  487000\n",
      "i =  487500\n",
      "i =  488000\n",
      "i =  488500\n",
      "i =  489000\n",
      "i =  489500\n",
      "i =  490000\n",
      "i =  490500\n",
      "i =  491000\n",
      "i =  491500\n",
      "i =  492000\n",
      "i =  492500\n",
      "i =  493000\n",
      "i =  493500\n",
      "i =  494000\n",
      "i =  494500\n",
      "i =  495000\n",
      "i =  495500\n",
      "i =  496000\n",
      "i =  496500\n",
      "i =  497000\n",
      "i =  497500\n",
      "i =  498000\n",
      "i =  498500\n",
      "i =  499000\n",
      "i =  499500\n",
      "i =  500000\n",
      "i =  500500\n",
      "i =  501000\n",
      "i =  501500\n",
      "i =  502000\n",
      "i =  502500\n",
      "i =  503000\n",
      "i =  503500\n",
      "i =  504000\n",
      "i =  504500\n",
      "i =  505000\n",
      "i =  505500\n",
      "i =  506000\n",
      "i =  506500\n",
      "i =  507000\n",
      "i =  507500\n",
      "i =  508000\n",
      "i =  508500\n",
      "i =  509000\n",
      "i =  509500\n",
      "i =  510000\n",
      "i =  510500\n",
      "i =  511000\n",
      "i =  511500\n",
      "i =  512000\n",
      "i =  512500\n",
      "i =  513000\n",
      "i =  513500\n",
      "i =  514000\n",
      "i =  514500\n",
      "i =  515000\n",
      "i =  515500\n",
      "i =  516000\n",
      "i =  516500\n",
      "i =  517000\n",
      "i =  517500\n",
      "i =  518000\n",
      "i =  518500\n",
      "i =  519000\n",
      "i =  519500\n",
      "i =  520000\n",
      "i =  520500\n",
      "i =  521000\n",
      "i =  521500\n",
      "i =  522000\n",
      "i =  522500\n",
      "i =  523000\n",
      "i =  523500\n",
      "i =  524000\n",
      "i =  524500\n",
      "i =  525000\n",
      "i =  525500\n",
      "i =  526000\n",
      "i =  526500\n",
      "i =  527000\n",
      "i =  527500\n",
      "i =  528000\n",
      "i =  528500\n",
      "i =  529000\n",
      "i =  529500\n",
      "i =  530000\n",
      "i =  530500\n",
      "i =  531000\n",
      "i =  531500\n",
      "i =  532000\n",
      "i =  532500\n",
      "i =  533000\n",
      "i =  533500\n",
      "i =  534000\n",
      "i =  534500\n",
      "i =  535000\n",
      "i =  535500\n",
      "i =  536000\n",
      "i =  536500\n",
      "i =  537000\n",
      "i =  537500\n",
      "i =  538000\n",
      "i =  538500\n",
      "i =  539000\n",
      "i =  539500\n",
      "i =  540000\n",
      "i =  540500\n",
      "i =  541000\n",
      "i =  541500\n",
      "i =  542000\n",
      "i =  542500\n",
      "i =  543000\n",
      "i =  543500\n",
      "i =  544000\n",
      "i =  544500\n",
      "i =  545000\n",
      "i =  545500\n",
      "i =  546000\n",
      "i =  546500\n",
      "i =  547000\n",
      "i =  547500\n",
      "i =  548000\n",
      "i =  548500\n",
      "i =  549000\n",
      "i =  549500\n",
      "i =  550000\n",
      "i =  550500\n",
      "i =  551000\n",
      "i =  551500\n",
      "i =  552000\n",
      "i =  552500\n",
      "i =  553000\n",
      "i =  553500\n",
      "i =  554000\n",
      "i =  554500\n",
      "i =  555000\n",
      "i =  555500\n",
      "i =  556000\n",
      "i =  556500\n",
      "i =  557000\n",
      "i =  557500\n",
      "i =  558000\n",
      "i =  558500\n",
      "i =  559000\n",
      "i =  559500\n",
      "i =  560000\n",
      "i =  560500\n",
      "i =  561000\n",
      "i =  561500\n",
      "i =  562000\n",
      "i =  562500\n",
      "i =  563000\n",
      "i =  563500\n",
      "i =  564000\n",
      "i =  564500\n",
      "i =  565000\n",
      "i =  565500\n",
      "i =  566000\n",
      "i =  566500\n",
      "i =  567000\n",
      "i =  567500\n",
      "i =  568000\n",
      "i =  568500\n",
      "i =  569000\n",
      "i =  569500\n",
      "i =  570000\n",
      "i =  570500\n",
      "i =  571000\n",
      "i =  571500\n",
      "i =  572000\n",
      "i =  572500\n",
      "i =  573000\n",
      "i =  573500\n",
      "i =  574000\n",
      "i =  574500\n",
      "i =  575000\n",
      "i =  575500\n",
      "i =  576000\n",
      "i =  576500\n",
      "i =  577000\n",
      "i =  577500\n",
      "i =  578000\n",
      "i =  578500\n",
      "i =  579000\n",
      "i =  579500\n",
      "i =  580000\n",
      "i =  580500\n",
      "i =  581000\n",
      "i =  581500\n",
      "i =  582000\n",
      "i =  582500\n",
      "i =  583000\n",
      "i =  583500\n",
      "i =  584000\n",
      "i =  584500\n",
      "i =  585000\n",
      "i =  585500\n",
      "i =  586000\n",
      "i =  586500\n",
      "i =  587000\n",
      "i =  587500\n",
      "i =  588000\n",
      "i =  588500\n",
      "i =  589000\n",
      "i =  589500\n",
      "i =  590000\n",
      "i =  590500\n",
      "i =  591000\n",
      "i =  591500\n",
      "i =  592000\n",
      "i =  592500\n",
      "i =  593000\n",
      "i =  593500\n",
      "i =  594000\n",
      "i =  594500\n",
      "i =  595000\n",
      "i =  595500\n",
      "i =  596000\n",
      "i =  596500\n",
      "i =  597000\n",
      "i =  597500\n",
      "i =  598000\n",
      "i =  598500\n",
      "i =  599000\n",
      "i =  599500\n",
      "i =  600000\n",
      "i =  600500\n",
      "i =  601000\n",
      "i =  601500\n",
      "i =  602000\n",
      "i =  602500\n",
      "i =  603000\n",
      "i =  603500\n",
      "i =  604000\n",
      "i =  604500\n",
      "i =  605000\n",
      "i =  605500\n",
      "i =  606000\n",
      "i =  606500\n",
      "i =  607000\n",
      "i =  607500\n",
      "i =  608000\n",
      "i =  608500\n",
      "i =  609000\n",
      "i =  609500\n",
      "i =  610000\n",
      "i =  610500\n",
      "i =  611000\n",
      "i =  611500\n",
      "i =  612000\n",
      "i =  612500\n",
      "i =  613000\n",
      "i =  613500\n",
      "i =  614000\n",
      "i =  614500\n",
      "i =  615000\n",
      "i =  615500\n",
      "i =  616000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  616500\n",
      "i =  617000\n",
      "i =  617500\n",
      "i =  618000\n",
      "i =  618500\n",
      "i =  619000\n",
      "i =  619500\n",
      "i =  620000\n",
      "i =  620500\n",
      "i =  621000\n",
      "i =  621500\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_X_y(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "Batch:  19\n",
      "Batch:  20\n",
      "Batch:  21\n",
      "Batch:  22\n",
      "68761\n",
      "i =  0\n",
      "i =  500\n",
      "i =  1000\n",
      "i =  1500\n",
      "i =  2000\n",
      "i =  2500\n",
      "i =  3000\n",
      "i =  3500\n",
      "i =  4000\n",
      "i =  4500\n",
      "i =  5000\n",
      "i =  5500\n",
      "i =  6000\n",
      "i =  6500\n",
      "i =  7000\n",
      "i =  7500\n",
      "i =  8000\n",
      "i =  8500\n",
      "i =  9000\n",
      "i =  9500\n",
      "i =  10000\n",
      "i =  10500\n",
      "i =  11000\n",
      "i =  11500\n",
      "i =  12000\n",
      "i =  12500\n",
      "i =  13000\n",
      "i =  13500\n",
      "i =  14000\n",
      "i =  14500\n",
      "i =  15000\n",
      "i =  15500\n",
      "i =  16000\n",
      "i =  16500\n",
      "i =  17000\n",
      "i =  17500\n",
      "i =  18000\n",
      "i =  18500\n",
      "i =  19000\n",
      "i =  19500\n",
      "i =  20000\n",
      "i =  20500\n",
      "i =  21000\n",
      "i =  21500\n",
      "i =  22000\n",
      "i =  22500\n",
      "i =  23000\n",
      "i =  23500\n",
      "i =  24000\n",
      "i =  24500\n",
      "i =  25000\n",
      "i =  25500\n",
      "i =  26000\n",
      "i =  26500\n",
      "i =  27000\n",
      "i =  27500\n",
      "i =  28000\n",
      "i =  28500\n",
      "i =  29000\n",
      "i =  29500\n",
      "i =  30000\n",
      "i =  30500\n",
      "i =  31000\n",
      "i =  31500\n",
      "i =  32000\n",
      "i =  32500\n",
      "i =  33000\n",
      "i =  33500\n",
      "i =  34000\n",
      "i =  34500\n",
      "i =  35000\n",
      "i =  35500\n",
      "i =  36000\n",
      "i =  36500\n",
      "i =  37000\n",
      "i =  37500\n",
      "i =  38000\n",
      "i =  38500\n",
      "i =  39000\n",
      "i =  39500\n",
      "i =  40000\n",
      "i =  40500\n",
      "i =  41000\n",
      "i =  41500\n",
      "i =  42000\n",
      "i =  42500\n",
      "i =  43000\n",
      "i =  43500\n",
      "i =  44000\n",
      "i =  44500\n",
      "i =  45000\n",
      "i =  45500\n",
      "i =  46000\n",
      "i =  46500\n",
      "i =  47000\n",
      "i =  47500\n",
      "i =  48000\n",
      "i =  48500\n",
      "i =  49000\n",
      "i =  49500\n",
      "i =  50000\n",
      "i =  50500\n",
      "i =  51000\n",
      "i =  51500\n",
      "i =  52000\n",
      "i =  52500\n",
      "i =  53000\n",
      "i =  53500\n",
      "i =  54000\n",
      "i =  54500\n",
      "i =  55000\n",
      "i =  55500\n",
      "i =  56000\n",
      "i =  56500\n",
      "i =  57000\n",
      "i =  57500\n",
      "i =  58000\n",
      "i =  58500\n",
      "i =  59000\n",
      "i =  59500\n",
      "i =  60000\n",
      "i =  60500\n",
      "i =  61000\n",
      "i =  61500\n",
      "i =  62000\n",
      "i =  62500\n",
      "i =  63000\n",
      "i =  63500\n",
      "i =  64000\n",
      "i =  64500\n",
      "i =  65000\n",
      "i =  65500\n",
      "i =  66000\n",
      "i =  66500\n",
      "i =  67000\n",
      "i =  67500\n",
      "i =  68000\n",
      "i =  68500\n"
     ]
    }
   ],
   "source": [
    "X_val, y_val = get_X_y(val_df, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = [torch.FloatTensor(item) for item in data]\n",
    "        self.targets = torch.LongTensor(targets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelEncoder()\n",
    "\n",
    "\n",
    "t = pd.DataFrame({'x': X_train, \n",
    "                   'y': y_train}).sample(frac=1).reset_index(drop=True)\n",
    "# t = t[~t.y.isin(['yes','no'])]\n",
    "\n",
    "v = pd.DataFrame({'x': X_val, \n",
    "                   'y': y_val}).sample(frac=1).reset_index(drop=True)\n",
    "# v = v[~v.y.isin(['yes','no'])]\n",
    "\n",
    "# x_t, x_v, y_t, y_v = train_test_split(x_no_binary_train.x, x_no_binary_train.y, test_size=0.10, random_state=42)\n",
    "\n",
    "x_t = t.x\n",
    "y_t = t.y\n",
    "y_t = enc.fit_transform(y_t)\n",
    "\n",
    "x_v = v.x\n",
    "y_v = v.y\n",
    "y_v = enc.transform(y_v)\n",
    "\n",
    "# X_test = x_no_binary_test.x\n",
    "# y_test = enc.transform(x_no_binary_test.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_t, x_v, y_t, y_v = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
    "\n",
    "# enc = LabelEncoder()\n",
    "\n",
    "# y_t = enc.fit_transform(y_t)\n",
    "# y_v = enc.transform(y_v)\n",
    "\n",
    "# X_test = X_val\n",
    "# y_test = enc.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_t, y_t)\n",
    "val_dataset = CustomDataset(x_v, y_v)\n",
    "test_dataset = CustomDataset(x_v, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSTEP 1: LOADING DATASET\\n'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "# train_dataset = dsets.MNIST(root='./data', \n",
    "#                             train=True, \n",
    "#                             transform=transforms.ToTensor(),\n",
    "#                             download=True)\n",
    "\n",
    "# test_dataset = dsets.MNIST(root='./data', \n",
    "#                            train=False, \n",
    "#                            transform=transforms.ToTensor())\n",
    "\n",
    "# train_dataset = CustomDataset(X_train, y_t)\n",
    "# test_dataset = CustomDataset(X_val, y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 512\n",
    "n_iters = 10000\n",
    "num_epochs = 20 #n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.140274294166828e-05"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iters / len(train_dataset)/ batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, h1, h2, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 2048 --> 1600\n",
    "        self.fc1 = nn.Linear(input_dim, h1) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "#         # Linear function 2: 1600 --> 1000\n",
    "#         self.fc2 = nn.Linear(h1, h2)\n",
    "#         # Non-linearity 2\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#         # Linear function 3: 100 --> 100\n",
    "#         self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         # Non-linearity 3\n",
    "#         self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Linear function 4 (readout): 1000 --> 754\n",
    "        self.fc4 = nn.Linear(h1, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "#         out = self.fc2(out)\n",
    "#         # Non-linearity 2\n",
    "#         out = self.relu2(out)\n",
    "\n",
    "#         # Linear function 2\n",
    "#         out = self.fc3(out)\n",
    "#         # Non-linearity 2\n",
    "#         out = self.relu3(out)\n",
    "\n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 2048\n",
    "h1 = 1600\n",
    "h2 = 1000\n",
    "output_dim = len(set(y_v))\n",
    "\n",
    "\n",
    "vqa_model = FeedforwardNeuralNetModel(input_dim, h1, h2, output_dim)\n",
    "# model = LogisticRegressionModel(input_dim, output_dim)\n",
    "\n",
    "#######################\n",
    "#  USE GPU FOR MODEL  #\n",
    "#######################\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "vqa_model.to(device)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(vqa_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(621960, 68761, 68761)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Iteration: 500. Loss: 0.8088433742523193. Accuracy: 77.31708381204461\n",
      "Iteration: 1000. Loss: 0.6553743481636047. Accuracy: 79.4447433865127\n",
      "Epoch:  1\n",
      "Iteration: 1500. Loss: 0.7887970209121704. Accuracy: 80.54565814924158\n",
      "Iteration: 2000. Loss: 0.7032681703567505. Accuracy: 81.2044618315615\n",
      "Epoch:  2\n",
      "Iteration: 2500. Loss: 0.6179889440536499. Accuracy: 81.5811288375678\n",
      "Iteration: 3000. Loss: 0.6018741726875305. Accuracy: 81.93452683934207\n",
      "Iteration: 3500. Loss: 0.48334023356437683. Accuracy: 82.10177280725993\n",
      "Epoch:  3\n",
      "Iteration: 4000. Loss: 0.5481206178665161. Accuracy: 81.92580096275505\n",
      "Iteration: 4500. Loss: 0.5482903122901917. Accuracy: 82.18903157313012\n",
      "Epoch:  4\n",
      "Iteration: 5000. Loss: 0.43066781759262085. Accuracy: 81.90398627128751\n",
      "Iteration: 5500. Loss: 0.4971775412559509. Accuracy: 82.28647052835183\n",
      "Iteration: 6000. Loss: 0.4635138511657715. Accuracy: 82.44644493244718\n",
      "Epoch:  5\n",
      "Iteration: 6500. Loss: 0.37151405215263367. Accuracy: 82.58896758336849\n",
      "Iteration: 7000. Loss: 0.47395145893096924. Accuracy: 82.4129957388636\n",
      "Epoch:  6\n",
      "Iteration: 7500. Loss: 0.32072219252586365. Accuracy: 82.55842701531391\n",
      "Iteration: 8000. Loss: 0.4587678611278534. Accuracy: 82.49007431538227\n",
      "Iteration: 8500. Loss: 0.4129582941532135. Accuracy: 82.66313753435814\n",
      "Epoch:  7\n",
      "Iteration: 9000. Loss: 0.36580097675323486. Accuracy: 82.46535099838572\n",
      "Iteration: 9500. Loss: 0.36569494009017944. Accuracy: 82.72712729599628\n",
      "Epoch:  8\n",
      "Iteration: 10000. Loss: 0.26640936732292175. Accuracy: 82.51334331961432\n",
      "Iteration: 10500. Loss: 0.344886839389801. Accuracy: 82.55551838978491\n",
      "Epoch:  9\n",
      "Iteration: 11000. Loss: 0.2955893874168396. Accuracy: 82.41154142609909\n",
      "Iteration: 11500. Loss: 0.2572615146636963. Accuracy: 82.28065327729381\n",
      "Iteration: 12000. Loss: 0.3194618225097656. Accuracy: 82.75912217681534\n",
      "Epoch:  10\n",
      "Iteration: 12500. Loss: 0.2842577397823334. Accuracy: 82.44353630691816\n",
      "Iteration: 13000. Loss: 0.37581127882003784. Accuracy: 82.38827242186704\n",
      "Epoch:  11\n",
      "Iteration: 13500. Loss: 0.2516040802001953. Accuracy: 82.46971393667923\n",
      "Iteration: 14000. Loss: 0.2712969183921814. Accuracy: 82.30973953258388\n",
      "Iteration: 14500. Loss: 0.26138293743133545. Accuracy: 82.21520920289117\n",
      "Epoch:  12\n",
      "Iteration: 15000. Loss: 0.2752102017402649. Accuracy: 82.60351071101351\n",
      "Iteration: 15500. Loss: 0.2753526568412781. Accuracy: 81.74401186719216\n",
      "Epoch:  13\n",
      "Iteration: 16000. Loss: 0.29521444439888. Accuracy: 82.37809223251553\n",
      "Iteration: 16500. Loss: 0.26469552516937256. Accuracy: 82.38245517080904\n",
      "Iteration: 17000. Loss: 0.27463963627815247. Accuracy: 82.25883858582627\n",
      "Epoch:  14\n",
      "Iteration: 17500. Loss: 0.24079453945159912. Accuracy: 82.0872296796149\n",
      "Iteration: 18000. Loss: 0.28411704301834106. Accuracy: 82.25156702200375\n",
      "Epoch:  15\n",
      "Iteration: 18500. Loss: 0.2182266265153885. Accuracy: 82.20648332630415\n",
      "Iteration: 19000. Loss: 0.20642012357711792. Accuracy: 82.16139963060456\n",
      "Epoch:  16\n",
      "Iteration: 19500. Loss: 0.23996126651763916. Accuracy: 82.07995811579238\n",
      "Iteration: 20000. Loss: 0.2092878818511963. Accuracy: 82.11631593490496\n",
      "Iteration: 20500. Loss: 0.21969705820083618. Accuracy: 82.10177280725993\n",
      "Epoch:  17\n",
      "Iteration: 21000. Loss: 0.18823999166488647. Accuracy: 81.54186239292622\n",
      "Iteration: 21500. Loss: 0.2111908197402954. Accuracy: 82.13085906254999\n",
      "Epoch:  18\n",
      "Iteration: 22000. Loss: 0.14187835156917572. Accuracy: 82.23411526882971\n",
      "Iteration: 22500. Loss: 0.21319293975830078. Accuracy: 82.15267375401754\n",
      "Iteration: 23000. Loss: 0.2128814458847046. Accuracy: 81.9912450371577\n",
      "Epoch:  19\n",
      "Iteration: 23500. Loss: 0.1324746012687683. Accuracy: 81.86762845217493\n",
      "Iteration: 24000. Loss: 0.16903893649578094. Accuracy: 82.03487442009279\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    for i, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        #######################\n",
    "        #  USE GPU FOR MODEL  #\n",
    "        #######################\n",
    "        features = features.requires_grad_().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = vqa_model(features)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for features, labels in val_loader:\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                features = features.to(device)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = vqa_model(features)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                #######################\n",
    "                #  USE GPU FOR MODEL  #\n",
    "                #######################\n",
    "                # Total correct predictions\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct.item() / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "            \n",
    "# print(\"Test Set\")\n",
    "# # Calculate Accuracy         \n",
    "# correct = 0\n",
    "# total = 0\n",
    "# # Iterate through test dataset\n",
    "# for features, labels in test_loader:\n",
    "#     #######################\n",
    "#     #  USE GPU FOR MODEL  #\n",
    "#     #######################\n",
    "#     features = features.to(device)\n",
    "\n",
    "#     # Forward pass only to get logits/outputpredicted\n",
    "#     outputs = vqa_model(features)\n",
    "\n",
    "#     # Get predictions from the maximum value\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "#     # Total number of labels\n",
    "#     total += labels.size(0)\n",
    "\n",
    "#     #######################\n",
    "#     #  USE GPU FOR MODEL  #\n",
    "#     #######################\n",
    "#     # Total correct predictions\n",
    "#     if torch.cuda.is_available():\n",
    "#         correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "#     else:\n",
    "#         correct += (predicted == labels).sum()\n",
    "\n",
    "# accuracy = 100 * correct.item() / total\n",
    "\n",
    "# # Print Loss\n",
    "# print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(vqa_, loader, k=1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in loader:\n",
    "        features = features.to(device)\n",
    "#         print(features.shape)\n",
    "        outputs = vqa_(features)\n",
    "        _, predicted = torch.topk(outputs.data, k=k, dim=1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        \n",
    "        for counter, label in enumerate(labels.cpu()):\n",
    "            correct+=label in predicted.cpu()[counter]\n",
    "        \n",
    "    accuracy = 100 * correct / total\n",
    "    print('Accuracy @ {}: {}'.format(k, accuracy))\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy @ 1: 81.98542778609968\n",
      "Accuracy @ 5: 94.48960893529762\n",
      "Accuracy @ 10: 96.05154084437399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "96.05154084437399"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_accuracy(vqa_model, test_loader, k = 1)\n",
    "top_k_accuracy(vqa_model, test_loader, k = 5)\n",
    "top_k_accuracy(vqa_model, test_loader, k = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "440\n",
      "i =  0\n",
      "Accuracy @ 1: 47.27272727272727\n",
      "Accuracy @ 5: 82.72727272727273\n",
      "Accuracy @ 10: 92.95454545454545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.95454545454545"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color = pd.read_csv(\"../data/ADE20K-QA/categorized/what_color_q_val.csv\")\n",
    "\n",
    "color = get_X_y(color, train=False)\n",
    "\n",
    "color = pd.DataFrame({'x': color[0], \n",
    "                   'y': enc.transform(color[1])}).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "color_dataset = CustomDataset(color.x, color.y)\n",
    "\n",
    "color_loader = torch.utils.data.DataLoader(dataset=color_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "top_k_accuracy(vqa_model, color_loader, k = 1)\n",
    "top_k_accuracy(vqa_model, color_loader, k = 5)\n",
    "top_k_accuracy(vqa_model, color_loader, k = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "Batch:  1\n",
      "Batch:  2\n",
      "Batch:  3\n",
      "Batch:  4\n",
      "Batch:  5\n",
      "Batch:  6\n",
      "Batch:  7\n",
      "Batch:  8\n",
      "Batch:  9\n",
      "Batch:  10\n",
      "Batch:  11\n",
      "Batch:  12\n",
      "Batch:  13\n",
      "Batch:  14\n",
      "Batch:  15\n",
      "Batch:  16\n",
      "Batch:  17\n",
      "Batch:  18\n",
      "56007\n",
      "i =  0\n",
      "i =  500\n",
      "i =  1000\n",
      "i =  1500\n",
      "i =  2000\n",
      "i =  2500\n",
      "i =  3000\n",
      "i =  3500\n",
      "i =  4000\n",
      "i =  4500\n",
      "i =  5000\n",
      "i =  5500\n",
      "i =  6000\n",
      "i =  6500\n",
      "i =  7000\n",
      "i =  7500\n",
      "i =  8000\n",
      "i =  8500\n",
      "i =  9000\n",
      "i =  9500\n",
      "i =  10000\n",
      "i =  10500\n",
      "i =  11000\n",
      "i =  11500\n",
      "i =  12000\n",
      "i =  12500\n",
      "i =  13000\n",
      "i =  13500\n",
      "i =  14000\n",
      "i =  14500\n",
      "i =  15000\n",
      "i =  15500\n",
      "i =  16000\n",
      "i =  16500\n",
      "i =  17000\n",
      "i =  17500\n",
      "i =  18000\n",
      "i =  18500\n",
      "i =  19000\n",
      "i =  19500\n",
      "i =  20000\n",
      "i =  20500\n",
      "i =  21000\n",
      "i =  21500\n",
      "i =  22000\n",
      "i =  22500\n",
      "i =  23000\n",
      "i =  23500\n",
      "i =  24000\n",
      "i =  24500\n",
      "i =  25000\n",
      "i =  25500\n",
      "i =  26000\n",
      "i =  26500\n",
      "i =  27000\n",
      "i =  27500\n",
      "i =  28000\n",
      "i =  28500\n",
      "i =  29000\n",
      "i =  29500\n",
      "i =  30000\n",
      "i =  30500\n",
      "i =  31000\n",
      "i =  31500\n",
      "i =  32000\n",
      "i =  32500\n",
      "i =  33000\n",
      "i =  33500\n",
      "i =  34000\n",
      "i =  34500\n",
      "i =  35000\n",
      "i =  35500\n",
      "i =  36000\n",
      "i =  36500\n",
      "i =  37000\n",
      "i =  37500\n",
      "i =  38000\n",
      "i =  38500\n",
      "i =  39000\n",
      "i =  39500\n",
      "i =  40000\n",
      "i =  40500\n",
      "i =  41000\n",
      "i =  41500\n",
      "i =  42000\n",
      "i =  42500\n",
      "i =  43000\n",
      "i =  43500\n",
      "i =  44000\n",
      "i =  44500\n",
      "i =  45000\n",
      "i =  45500\n",
      "i =  46000\n",
      "i =  46500\n",
      "i =  47000\n",
      "i =  47500\n",
      "i =  48000\n",
      "i =  48500\n",
      "i =  49000\n",
      "i =  49500\n",
      "i =  50000\n",
      "i =  50500\n",
      "i =  51000\n",
      "i =  51500\n",
      "i =  52000\n",
      "i =  52500\n",
      "i =  53000\n",
      "i =  53500\n",
      "i =  54000\n",
      "i =  54500\n",
      "i =  55000\n",
      "i =  55500\n",
      "i =  56000\n",
      "Accuracy @ 1: 90.87971146463835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90.87971146463835"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary = pd.read_csv(\"../data/ADE20K-QA/categorized/binary_q_val.csv\")\n",
    "\n",
    "binary = get_X_y(binary, train=False)\n",
    "\n",
    "binary = pd.DataFrame({'x': binary[0], \n",
    "                   'y': enc.transform(binary[1])}).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "binary_dataset = CustomDataset(binary.x, binary.y)\n",
    "\n",
    "binary_loader = torch.utils.data.DataLoader(dataset=binary_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "top_k_accuracy(vqa_model, binary_loader, k = 1)\n",
    "# top_k_accuracy(vqa_model, binary_loader, k = 5)\n",
    "# top_k_accuracy(vqa_model, binary_loader, k = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "713\n",
      "i =  0\n",
      "i =  500\n",
      "Accuracy @ 1: 62.41234221598878\n",
      "Accuracy @ 5: 83.16970546984572\n",
      "Accuracy @ 10: 87.9382889200561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "87.9382889200561"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "where = pd.read_csv(\"../data/ADE20K-QA/categorized/where_q_val.csv\")\n",
    "\n",
    "where = get_X_y(where, train=False)\n",
    "\n",
    "where = pd.DataFrame({'x': where[0], \n",
    "                   'y': enc.transform(where[1])}).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "where_dataset = CustomDataset(where.x, where.y)\n",
    "\n",
    "where_dataloader = torch.utils.data.DataLoader(dataset=where_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "top_k_accuracy(vqa_model, where_dataloader, k = 1)\n",
    "top_k_accuracy(vqa_model, where_dataloader, k = 5)\n",
    "top_k_accuracy(vqa_model, where_dataloader, k = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  0\n",
      "256\n",
      "i =  0\n",
      "Accuracy @ 1: 47.265625\n",
      "Accuracy @ 5: 83.984375\n",
      "Accuracy @ 10: 92.1875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.1875"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "how_many = pd.read_csv(\"../data/ADE20K-QA/categorized/how_many_q_val.csv\")\n",
    "\n",
    "how_many = get_X_y(how_many, train=False)\n",
    "\n",
    "how_many = pd.DataFrame({'x': how_many[0], \n",
    "                   'y': enc.transform(how_many[1])}).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "how_many_dataset = CustomDataset(how_many.x, how_many.y)\n",
    "\n",
    "how_many_dataloader = torch.utils.data.DataLoader(dataset=how_many_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "top_k_accuracy(vqa_model, how_many_dataloader, k = 1)\n",
    "top_k_accuracy(vqa_model, how_many_dataloader, k = 5)\n",
    "top_k_accuracy(vqa_model, how_many_dataloader, k = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['building', 'one', 'white', 'chair', 'tree', 'rack',\n",
       "       'glass window', 'board', 'cabinet', 'candle stand', 'candle stand',\n",
       "       'kitchen platform', 'wall', 'tree', 'cloud', 'sky', 'floor',\n",
       "       'floor', 'bush', 'floor', 'board', 'rock', 'table', 'cloudy',\n",
       "       'stone', 'door', 'pillow', 'plant', 'table', 'dustbin', 'city',\n",
       "       'right', 'pillow', 'chair', 'rod', 'monitor', 'flower vase',\n",
       "       'right', 'poster', 'building', 'road', 'table', 'paper', 'middle',\n",
       "       'glass window', 'white', 'tree', 'window', 'roof', 'door', 'lamp',\n",
       "       'small', 'floor', 'table', 'grass', 'pillar', 'cupboard',\n",
       "       'chimney', 'food', 'wooden', 'flush tank', 'frame', 'cloud',\n",
       "       'roof', 'floor', 'two', 'sky', 'flower', 'glass', 'candle',\n",
       "       'cloud', 'glass', 'building', 'road', 'lamp', 'sofa chair', 'wire',\n",
       "       'decor', 'pole', 'brown', 'desk', 'light', 'text', 'photo',\n",
       "       'building', 'sky', 'frame', 'right', 'floor', 'wall', 'table',\n",
       "       'yellow', 'number', 'person', 'chess', 'sofa', 'stool', 'building',\n",
       "       'middle', 'window', 'bathroom', 'frame', 'roof', 'bathtub', 'wall',\n",
       "       'white', 'floor', 'green grass', 'right', 'stair', 'person',\n",
       "       'pole', 'rod', 'water', 'sky', 'table', 'table', 'right', 'car',\n",
       "       'wash basin', 'light', 'room', 'road', 'building', 'wall', 'door',\n",
       "       'pavement', 'carpet', 'tree', 'rope', 'sky', 'sky', 'pole', 'book',\n",
       "       'flower vase', 'cloudy', 'window', 'bed', 'front', 'table', 'sky',\n",
       "       'curtain', 'vehicle', 'glass door', 'board', 'cross symbol',\n",
       "       'iron', 'roof', 'pole', 'light', 'table', 'black', 'sky',\n",
       "       'vehicle', 'flower bouquet', 'window', 'table', 'oven', 'wall',\n",
       "       'road', 'man', 'auditorium', 'tree', 'sky', 'wooden', 'bicycle',\n",
       "       'statue', 'snooker', 'concrete', 'room', 'sky', 'window', 'stone',\n",
       "       'bed', 'frame', 'sofa', 'floor', 'chair', 'floor', 'frame',\n",
       "       'window', 'hand', 'chandelier', 'vehicle', 'text', 'bench', 'boat',\n",
       "       'room', 'wall', 'cloudy', 'wall', 'right', 'sky', 'right', 'color',\n",
       "       'glass', 'wooden', 'bike', 'lamp', 'middle', 'floor', 'front',\n",
       "       'door', 'road', 'room', 'tree', 'table', 'people', 'bed', 'window',\n",
       "       'laptop', 'big', 'bed', 'sky', 'roof', 'middle', 'room', 'black',\n",
       "       'sky', 'tile', 'mirror', 'car', 'building', 'brown', 'frame',\n",
       "       'human', 'vehicle', 'grass', 'sky', 'table', 'oven', 'one', 'tree',\n",
       "       'people', 'vehicle', 'building', 'clothe', 'two', 'stair', 'back',\n",
       "       'snowy mountain', 'clothe', 'vase', 'wall', 'board', 'plate',\n",
       "       'wall', 'tree', 'number', 'wall', 'sky', 'people', 'ceiling',\n",
       "       'shed', 'cloudy', 'light', 'fruit', 'vehicle', 'mirror', 'color',\n",
       "       'television', 'glass', 'paper', 'photo', 'light', 'grass', 'grass',\n",
       "       'table', 'middle', 'table', 'table', 'wall', 'different', 'tree',\n",
       "       'two', 'glass', 'architecture', 'rack', 'white', 'sky', 'store',\n",
       "       'hill', 'screen', 'window', 'man', 'sky', 'people', 'glass',\n",
       "       'wooden', 'person', 'poster', 'tree', 'sky', 'car', 'plant',\n",
       "       'candle', 'building', 'metal rod', 'water', 'hydrant', 'cloud',\n",
       "       'sky', 'rack', 'sky', 'sky', 'roof', 'tree', 'water', 'sky',\n",
       "       'wall', 'mirror', 'inflatable', 'chair', 'iron', 'wooden', 'sky',\n",
       "       'curtain', 'chair', 'store', 'car', 'building', 'chair', 'roof',\n",
       "       'green grass', 'tree', 'building', 'front', 'person', 'tree',\n",
       "       'room', 'text', 'cloud', 'hill', 'person', 'dress', 'floor',\n",
       "       'road', 'person', 'chair', 'wire', 'frame', 'board', 'floor',\n",
       "       'ceiling', 'wall', 'people', 'girl', 'flower pot', 'bag', 'wooden',\n",
       "       'middle', 'surface', 'ball', 'blurry', 'dog', 'stone', 'sky',\n",
       "       'sky', 'chair', 'right', 'wall', 'chair', 'dark', 'hand', 'carpet',\n",
       "       'plant', 'monitor', 'pole', 'trunk', 'floor', 'front', 'middle',\n",
       "       'bench', 'pole', 'pillar', 'hanger', 'mirror', 'wooden',\n",
       "       'gym equipment', 'pillow', 'bag', 'frame', 'frame', 'right',\n",
       "       'blind', 'middle', 'roof', 'sofa', 'window', 'chair', 'box',\n",
       "       'plant', 'wash', 'floor', 'water', 'table', 'middle', 'curtain',\n",
       "       'yellow', 'table', 'white', 'pipe', 'glass', 'stand', 'tree',\n",
       "       'roof', 'rack', 'text', 'room', 'mirror', 'front', 'pipe', 'sky',\n",
       "       'surface', 'front', 'flower', 'mountain', 'shed', 'roof', 'chair',\n",
       "       'clothe', 'table', 'wall', 'tree', 'lamp', 'curtain', 'car',\n",
       "       'tree', 'tree', 'pillar', 'cloud', 'wire', 'table', 'bag', 'chair',\n",
       "       'footpath', 'ceiling', 'table', 'people', 'rod', 'frame', 'chair',\n",
       "       'building', 'window', 'table', 'front', 'cupboard', 'lamp',\n",
       "       'camera', 'floor', 'building', 'path', 'white', 'chandelier',\n",
       "       'visible', 'wooden', 'glass', 'tile', 'chair', 'floor mat',\n",
       "       'table', 'decorative', 'platform', 'building', 'tree', 'flower'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.inverse_transform(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tree', 'two', 'white', 'person', 'tree', 'shed', 'blind',\n",
       "       'button', 'stool', 'candle', 'light', 'kitchen', 'wall', 'water',\n",
       "       'cloud', 'sky', 'floor', 'mat', 'tree', 'pipe', 'board', 'rock',\n",
       "       'table', 'cloudy', 'tree', 'door', 'blanket', 'building', 'table',\n",
       "       'pole', 'city', 'right', 'table', 'bottle', 'big', 'screen',\n",
       "       'flower', 'right', 'board', 'number', 'road', 'table', 'frame',\n",
       "       'front', 'glass', 'white', 'tree', 'curtain', 'ceiling', 'cabin',\n",
       "       'bed', 'small', 'floor', 'lamp', 'water', 'floor', 'curtain',\n",
       "       'tree', 'tin', 'wooden', 'flush tank', 'pillar', 'cloud', 'roof',\n",
       "       'front', 'two', 'sky', 'flower', 'glass', 'candle', 'cloud',\n",
       "       'trolley', 'building', 'road', 'pen holder', 'lamp', 'bathtub',\n",
       "       'floor', 'footpath', 'red', 'center', 'light', 'card', 'frame',\n",
       "       'fort', 'sky', 'frame', 'right', 'table', 'floor', 'basket',\n",
       "       'pink', 'two', 'man', 'chess', 'couch', 'chair', 'area', 'front',\n",
       "       'window', 'bathroom', 'lamp', 'floor', 'towel', 'pole', 'white',\n",
       "       'corner', 'grass', 'right', 'stair', 'woman', 'chair', 'board',\n",
       "       'pole', 'water', 'door', 'telephone', 'right', 'car', 'wash basin',\n",
       "       'vessel', 'room', 'court', 'building', 'wall', 'wall', 'text',\n",
       "       'carpet', 'tree', 'roof', 'sky', 'sky', 'pavement', 'teapoy',\n",
       "       'cloth', 'clear', 'window', 'bed', 'front', 'table', 'sky',\n",
       "       'floor', 'vehicle', 'window', 'sign board', 'logo', 'iron', 'roof',\n",
       "       'board', 'text', 'dining table', 'red', 'sky', 'pillar',\n",
       "       'flower bouquet', 'land', 'sheet', 'bottle', 'book', 'road', 'man',\n",
       "       'ground', 'people', 'wall', 'glass', 'bag', 'wheel', 'pool',\n",
       "       'steel', 'bedroom', 'sky', 'window', 'stone', 'drawer', 'board',\n",
       "       'couch', 'floor', 'monitor', 'people', 'frame', 'pole', 'pole',\n",
       "       'chandelier', 'building', 'screen', 'bench', 'boat', 'church',\n",
       "       'front', 'blue', 'wall', 'right', 'sky', 'right', 'color', 'glass',\n",
       "       'metal', 'car', 'lamp', 'middle', 'wall', 'front', 'door', 'road',\n",
       "       'hall', 'tree', 'musical instrument', 'person', 'lamp', 'window',\n",
       "       'right', 'small', 'chair', 'building', 'front', 'middle', 'house',\n",
       "       'grey', 'river', 'handle', 'machine', 'vehicle', 'building',\n",
       "       'white', 'frame', 'people', 'vehicle', 'fence', 'rock', 'table',\n",
       "       'cupboard', 'traffic', 'road', 'card', 'vehicle', 'building',\n",
       "       'lamp', 'two', 'chair', 'right', 'snowy mountain', 'front', 'bowl',\n",
       "       'corner', 'glass', 'plate', 'wall', 'tree', 'frame', 'wall', 'sky',\n",
       "       'woman', 'ceiling', 'house', 'clear', 'light', 'box', 'plane',\n",
       "       'text', 'basket', 'light lamp', 'glass', 'paper', 'switchboard',\n",
       "       'light', 'green grass', 'pavement', 'chair', 'middle', 'table',\n",
       "       'architecture', 'wall', 'red', 'sky', 'three', 'pillar', 'water',\n",
       "       'shelf', 'white', 'sky', 'front', 'tree', 'board', 'window', 'man',\n",
       "       'sky', 'person', 'glass bottle', 'hand railing', 'person', 'clock',\n",
       "       'tree', 'sky', 'road', 'bottle', 'candle', 'shed', 'metal rod',\n",
       "       'sand', 'dustbin', 'sky', 'cloud', 'wall', 'sky', 'building',\n",
       "       'people', 'sky', 'vessel', 'vehicle', 'wall', 'candle stand',\n",
       "       'inflatable', 'curtain', 'rock', 'metal', 'cloud', 'curtain',\n",
       "       'chair', 'road', 'wall', 'building', 'shelf', 'roof',\n",
       "       'green grass', 'water', 'kitchen', 'middle', 'person', 'plant',\n",
       "       'room', 'word', 'cloud', 'mountain', 'person', 'spectacle',\n",
       "       'floor', 'road', 'person', 'chair', 'sign board', 'frame', 'sky',\n",
       "       'lamp', 'ceiling', 'wall', 'building', 'people', 'flower pot',\n",
       "       'hut', 'wooden', 'middle', 'ground', 'table', 'blurry', 'dog',\n",
       "       'flag', 'sky', 'road', 'car', 'right', 'wall', 'chair', 'dark',\n",
       "       'hand', 'floor', 'leave', 'board', 'road', 'trunk', 'floor',\n",
       "       'stand', 'middle', 'bench', 'pole', 'pillar', 'hanger', 'mirror',\n",
       "       'glass', 'gym equipment', 'table', 'hand', 'wall', 'frame',\n",
       "       'right', 'blind', 'middle', 'roof', 'sofa', 'wall', 'plant', 'tub',\n",
       "       'pillar', 'light', 'floor', 'water', 'chair', 'front',\n",
       "       'iron grille', 'green', 'desk', 'black', 'balcony', 'sculpture',\n",
       "       'bottle', 'tree', 'light', 'basket', 'text', 'room', 'window',\n",
       "       'ground', 'railing', 'cloud', 'sky', 'water', 'flower',\n",
       "       'water surface', 'shelter', 'middle', 'empty', 'wall', 'chair',\n",
       "       'wall', 'star', 'toy', 'curtain', 'wooden', 'walkway', 'rod',\n",
       "       'wall', 'ceiling', 'cup', 'table', 'dress', 'chair', 'footpath',\n",
       "       'ceiling', 'pillow', 'people', 'hanger', 'metal frame', 'grass',\n",
       "       'building', 'cupboard', 'pillar', 'bridge', 'wall', 'light',\n",
       "       'board', 'floor', 'tree', 'bridge', 'white', 'sculpture',\n",
       "       'visible', 'wooden', 'window blind', 'frame', 'chair', 'door mat',\n",
       "       'table', 'cloth', 'dustbin', 'cave', 'pavement', 'arch'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.inverse_transform(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "xxxx = dsets.MNIST(root='~/.cache/', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(xxxx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.models.regression import LogisticRegression\n",
    "from pl_bolts.datamodules import SklearnDataModule\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "enc = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([694, 389, 678, ..., 443, 443, 443])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = enc.fit_transform(y_train)\n",
    "y_val = enc.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dm = SklearnDataModule(X = X_train, y = y_train, x_test = X_val, y_test = y_val, \n",
    "                       val_split=0.1, test_split=0, num_workers=12, batch_size=256)\n",
    "# y_val = enc.transform(y_val)\n",
    "# dm_v = SklearnDataModule(X_val, y_val)\n",
    "\n",
    "model = LogisticRegression(input_dim=2048, num_classes=len(set(y_train)), learning_rate=0.001)\n",
    "model.train_dataloader = dm.train_dataloader\n",
    "model.val_dataloader = dm.val_dataloader\n",
    "model.test_dataloader = dm.test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Using native 16bit precision.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name   | Type   | Params\n",
      "----------------------------------\n",
      "0 | linear | Linear | 1.5 M \n",
      "----------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.196     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff8fe77d48f4d56b6a3b6fb96decee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=\"1\", precision=16, max_epochs=5)\n",
    "\n",
    "trainer.fit(model)\n",
    "# trainer.fit(model, train_dataloader=dm_t.train_dataloader())#, val_dataloader=dm_t.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ragarwal/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d125c114974cb2ba47ca29c4f662a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def test_epoch_end(self, outputs): \n",
    "#       results = process_outputs(outputs)\n",
    "#       self.test_results = results\n",
    "#       return results \n",
    "\n",
    "# trainer.test(model, datamodule)\n",
    "# results = model.test_results\n",
    "\n",
    "trainer.test(model,test_dataloaders=dm.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cuml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f1521c14492d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcuml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMulticlassClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cuml'"
     ]
    }
   ],
   "source": [
    "from cuml.linear_model import LogisticRegression\n",
    "\n",
    "from cuml.multiclass import MulticlassClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1, solver='saga')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_val)\n",
    "accuracy = np.mean((y_val == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
